{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"IceVision: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you Why IceVision? IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly Quick Example: How to train the PETS Dataset Source Code Happy Learning! If you need any assistance, feel free to: Join our Forum","title":"Home"},{"location":"#_1","text":"","title":""},{"location":"#_2","text":"IceVision: Agnostic Object Detection Framework We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Join our Forum We Need Your Help If you find this work useful, please let other people know by starring it, and sharing it on GitHub . Thank you","title":""},{"location":"#why-icevision","text":"IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly","title":"Why IceVision?"},{"location":"#quick-example-how-to-train-the-pets-dataset","text":"Source Code","title":"Quick Example: How to train the PETS Dataset"},{"location":"#happy-learning","text":"If you need any assistance, feel free to: Join our Forum","title":"Happy Learning!"},{"location":"IceApp_coco/","text":"IceVision Deployment App: COCO Dataset This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"COCO"},{"location":"IceApp_coco/#icevision-deployment-app-coco-dataset","text":"This example uses Faster RCNN trained weights using the COCO dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  COCO Dataset"},{"location":"IceApp_coco/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_coco/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_coco/#loading-trained-model","text":"class_map = icedata.coco.class_map() model = icedata.coco.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_coco/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_coco/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_coco/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - COCO') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://39710.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_coco/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_masks/","text":"IceVision Deployment App: PennFudan Dataset This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0] Defining the get_masks method: called by gr.Interface(fn=get_masks, ...) def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface # Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"Masks"},{"location":"IceApp_masks/#icevision-deployment-app-pennfudan-dataset","text":"This example uses Faster RCNN trained weights using the PennFudan dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App:  PennFudan Dataset"},{"location":"IceApp_masks/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_masks/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_masks/#loading-trained-model","text":"class_map = icedata.pennfudan.class_map() model = icedata.pennfudan.trained_models.mask_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_masks/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = mask_rcnn.build_infer_batch(infer_ds) preds = mask_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold, mask_threshold=mask_threshold, ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_masks/#defining-the-get_masks-method-called-by-grinterfacefnget_masks","text":"def get_masks(input_image, display_list, detection_threshold, mask_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) display_mask = (\"Mask\" in display_list) if detection_threshold==0: detection_threshold=0.5 if mask_threshold==0: mask_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold, mask_threshold=mask_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox, display_mask=display_mask) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the get_masks method: called by gr.Interface(fn=get_masks, ...)"},{"location":"IceApp_masks/#gradio-user-interface","text":"# Defining controls display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\", \"Mask\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") mask_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Mask Threshold\") # Setting outputs outputs = gr.outputs.Image(type=\"pil\") # Creating the user-interface gr_interface = gr.Interface(fn=get_masks, inputs=[\"image\", display_chkbox, detection_threshold_slider, mask_threshold_slider], outputs=outputs, title='IceApp - Masks') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://22314.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_masks/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"IceApp_pets/","text":"IceVision Deployment App: PETS Dataset This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones Installing packages !pip install icevision[inference] !pip install icedata !pip install gradio Imports from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr Loading trained model class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn() Defininig the predict() method def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0] Defining the show_preds method: called by gr.Interface(fn=show_preds, ...) def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img Gradio User Interface display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app Enjoy! If you have any questions, please feel free to join us","title":"PETS"},{"location":"IceApp_pets/#icevision-deployment-app-pets-dataset","text":"This example uses Faster RCNN trained weights using the PETS dataset About IceVision: an Object-Detection Framework that connects to different libraries/frameworks such as Fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) Provides flexible model implementations with pluggable backbones","title":"IceVision Deployment App: PETS Dataset"},{"location":"IceApp_pets/#installing-packages","text":"!pip install icevision[inference] !pip install icedata !pip install gradio","title":"Installing packages"},{"location":"IceApp_pets/#imports","text":"from icevision.all import * import icedata import PIL, requests import torch from torchvision import transforms import gradio as gr","title":"Imports"},{"location":"IceApp_pets/#loading-trained-model","text":"class_map = icedata.pets.class_map() model = icedata.pets.trained_models.faster_rcnn_resnet50_fpn()","title":"Loading trained model"},{"location":"IceApp_pets/#defininig-the-predict-method","text":"def predict( model, image, detection_threshold: float = 0.5, mask_threshold: float = 0.5 ): tfms_ = tfms.A.Adapter([tfms.A.Normalize()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset.from_images([image], tfms_) batch, samples = faster_rcnn.build_infer_batch(infer_ds) preds = faster_rcnn.predict( model=model, batch=batch, detection_threshold=detection_threshold ) return samples[0][\"img\"], preds[0]","title":"Defininig the predict() method"},{"location":"IceApp_pets/#defining-the-show_preds-method-called-by-grinterfacefnshow_preds","text":"def show_preds(input_image, display_list, detection_threshold): display_label = (\"Label\" in display_list) display_bbox = (\"BBox\" in display_list) if detection_threshold==0: detection_threshold=0.5 img, pred = predict(model=model, image=input_image, detection_threshold=detection_threshold) # print(pred) img = draw_pred(img=img, pred=pred, class_map=class_map, denormalize_fn=denormalize_imagenet, display_label=display_label, display_bbox=display_bbox) img = PIL.Image.fromarray(img) # print(\"Output Image: \", img.size, type(img)) return img","title":"Defining the show_preds method: called by gr.Interface(fn=show_preds, ...)"},{"location":"IceApp_pets/#gradio-user-interface","text":"display_chkbox = gr.inputs.CheckboxGroup([\"Label\", \"BBox\"], label=\"Display\") detection_threshold_slider = gr.inputs.Slider(minimum=0, maximum=1, step=0.1, default=0.5, label=\"Detection Threshold\") outputs = gr.outputs.Image(type=\"pil\") gr_interface = gr.Interface(fn=show_preds, inputs=[\"image\", display_chkbox, detection_threshold_slider], outputs=outputs, title='IceApp - PETS') gr_interface.launch(inline=False, share=True, debug=True) Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch(). This share link will expire in 6 hours. If you need a permanent link, email support@gradio.app Running on External URL: https://28865.gradio.app","title":"Gradio User Interface"},{"location":"IceApp_pets/#enjoy","text":"If you have any questions, please feel free to join us","title":"Enjoy!"},{"location":"about/","text":"Hall of Fame This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"About"},{"location":"about/#hall-of-fame","text":"This library is only made possible because of @all-contributors, thank you \u2665\ufe0f \u2665\ufe0f \u2665\ufe0f","title":"Hall of Fame"},{"location":"albumentations/","text":"Transforms Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases. Usage In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Albumentations"},{"location":"albumentations/#transforms","text":"Source Transforms are used in the following context: Resize and pad images to be fed to a given model, Augment the number of images in dataset that a given model will be train on. The augmented images are transformed images that will help the model to be trained on more diverse images, and consequently obtain a more robust trained model that will generally perform better than a model trained with non-augmented images, All the transforms are lazy transforms meaning they are applied on-the-fly: in other words, we do not create static transformed images which would increase the storage space IceVision Transforms Implementation: IceVision lays the foundation to easily integrate different augmentation libraries by using adapters. Out-of-the-box, it implements an adapter for the popular Albumentations library. Most of the examples and notebooks that we provide showcase how to use our Albumentations transforms. In addition, IceVision offers the users the option to create their own adapters using the augmentation library of their choice. They can follow a similar approach to the one we use to create their own augmentation library adapter. To ease the users' learning curve, we also provide the aug_tfms function that includes some of the most used transforms. The users can also override the default arguments. Other similar transforms pipeline can also be created by the users in order to be applied to their own use-cases.","title":"Transforms"},{"location":"albumentations/#usage","text":"In the following example, we highlight some of the most common usage of transforms. Transforms are used when we create both the train and valid Dataset objects. We often apply different transforms for the train and valid Dataset objects. Train transforms are used to augment the original dataset whereas valid transfoms are used to resize an image to fit the size the model expect. Example: Source In this example, there are two points to highlight: The train_tfms uses the predefined Albumentations transforms to augment the dataset during the train phase. They are applied on-the-fly (lazy transforms) The valid_tfms serves to resize validation images to the size the model expect # Defining transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ( [ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()] ) # Creating both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Original Image: Transformed Images: Note Notice how different transforms are applied to the original image. All the transformed have the same size despite applying some crop transforms. The size is preserved by adding padding (grey area)","title":"Usage"},{"location":"albumentations_tfms/","text":"[source] aug_tfms icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 45 , 45 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 20 , 20 ), g_shift_limit = ( - 20 , 20 ), b_shift_limit = ( - 20 , 20 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (height, width). presizing : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source] Adapter icevision . tfms . albumentations . Adapter ( tfms ) Adapter that enables the use of albumentations transforms. Arguments tfms Sequence[albumentations.core.transforms_interface.BasicTransform] : Sequence of albumentation transforms.","title":"Albumentations"},{"location":"albumentations_tfms/#aug_tfms","text":"icevision . tfms . albumentations . aug_tfms ( size , presize = None , horizontal_flip = HorizontalFlip ( always_apply = False , p = 0.5 ), shift_scale_rotate = ShiftScaleRotate ( always_apply = False , p = 0.5 , shift_limit = ( - 0.0625 , 0.0625 ), scale_limit = ( - 0.09999999999999998 , 0.10000000000000009 ), rotate_limit = ( - 45 , 45 ), interpolation = 1 , border_mode = 4 , value = None , mask_value = None , ), rgb_shift = RGBShift ( always_apply = False , p = 0.5 , r_shift_limit = ( - 20 , 20 ), g_shift_limit = ( - 20 , 20 ), b_shift_limit = ( - 20 , 20 ) ), lightning = RandomBrightnessContrast ( always_apply = False , p = 0.5 , brightness_limit = ( - 0.2 , 0.2 ), contrast_limit = ( - 0.2 , 0.2 ), brightness_by_max = True , ), blur = Blur ( always_apply = False , p = 0.5 , blur_limit = ( 1 , 3 )), crop_fn = functools . partial ( RandomSizedBBoxSafeCrop , p = 0.5 ), pad = functools . partial ( PadIfNeeded , border_mode = 0 , value = [ 124 , 116 , 104 ]), ) Collection of useful augmentation transforms. Arguments size Union[int, Tuple[int, int]] : The final size of the image. If an int is given, the maximum size of the image is rescaled, maintaing aspect ratio. If a tuple is given, the image is rescaled to have that exact size (height, width). presizing : Rescale the image before applying other transfroms. If None this transform is not applied. First introduced by fastai,this technique is explained in their book in this chapter (tip: search for \"Presizing\"). horizontal_flip Optional[albumentations.augmentations.transforms.HorizontalFlip] : Flip around the y-axis. If None this transform is not applied. shift_scale_rotate Optional[albumentations.augmentations.transforms.ShiftScaleRotate] : Randomly shift, scale, and rotate. If None this transform is not applied. rgb_shift Optional[albumentations.augmentations.transforms.RGBShift] : Randomly shift values for each channel of RGB image. If None this transform is not applied. lightning Optional[albumentations.augmentations.transforms.RandomBrightnessContrast] : Randomly changes Brightness and Contrast. If None this transform is not applied. blur Optional[albumentations.augmentations.transforms.Blur] : Randomly blur the image. If None this transform is not applied. crop_fn Optional[albumentations.core.transforms_interface.DualTransform] : Randomly crop the image. If None this transform is not applied. Use partial to saturate other parameters of the class. pad Optional[albumentations.core.transforms_interface.DualTransform] : Pad the image to size , squaring the image if size is an int . If None this transform is not applied. Use partial to sature other parameters of the class. Returns A list of albumentations transforms. [source]","title":"aug_tfms"},{"location":"albumentations_tfms/#adapter","text":"icevision . tfms . albumentations . Adapter ( tfms ) Adapter that enables the use of albumentations transforms. Arguments tfms Sequence[albumentations.core.transforms_interface.BasicTransform] : Sequence of albumentation transforms.","title":"Adapter"},{"location":"backbones_effecientdet/","text":"EffecientDet Backbones Source Usage We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. We can choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) EffecientDet Backbones Examples tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) Supported Backbones EffecientDet Backbones tf_efficientdet_lite0 efficientdet_d0 efficientdet_d1 efficientdet_d2 efficientdet_d3 efficientdet_d4 efficientdet_d5 efficientdet_d6 efficientdet_d7 efficientdet_d7x MobileNetv3 MNasNet MobileNetV2 MixNet","title":"EffecientDet"},{"location":"backbones_effecientdet/#effecientdet-backbones","text":"Source","title":"EffecientDet Backbones"},{"location":"backbones_effecientdet/#usage","text":"We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. We can choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more)","title":"Usage"},{"location":"backbones_effecientdet/#effecientdet-backbones-examples","text":"tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size )","title":"EffecientDet Backbones Examples"},{"location":"backbones_effecientdet/#supported-backbones","text":"EffecientDet Backbones tf_efficientdet_lite0 efficientdet_d0 efficientdet_d1 efficientdet_d2 efficientdet_d3 efficientdet_d4 efficientdet_d5 efficientdet_d6 efficientdet_d7 efficientdet_d7x MobileNetv3 MNasNet MobileNetV2 MixNet","title":"Supported Backbones"},{"location":"backbones_faster_mask_rcnn/","text":"Faster RCNN / Mask RCNN Backbones Source Usage We use the torchvision Faster RCNN model, and the torchvision Mask RCNN model. Both models accept a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the many backbones listed here below: Faster RCNN Backbones Examples fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet18 Example: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) Mask RCNN Backbones Examples fasterrcnn_resnet50_fpn Example: - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet34 Example: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) Supported Backbones FPN backbones - resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 Resnet backbone - resnet18 resnet34 resnet50 resnet101 resnet152 resnext101_32x8d MobileNet - mobilenet VGG vgg11 vgg13 vgg16 vgg19","title":"Faster/Mask RCNN"},{"location":"backbones_faster_mask_rcnn/#faster-rcnn-mask-rcnn-backbones","text":"Source","title":"Faster RCNN / Mask RCNN Backbones"},{"location":"backbones_faster_mask_rcnn/#usage","text":"We use the torchvision Faster RCNN model, and the torchvision Mask RCNN model. Both models accept a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the many backbones listed here below:","title":"Usage"},{"location":"backbones_faster_mask_rcnn/#faster-rcnn-backbones-examples","text":"fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet18 Example: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) )","title":"Faster RCNN Backbones Examples"},{"location":"backbones_faster_mask_rcnn/#mask-rcnn-backbones-examples","text":"fasterrcnn_resnet50_fpn Example: - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) resnet34 Example: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) )","title":"Mask RCNN Backbones Examples"},{"location":"backbones_faster_mask_rcnn/#supported-backbones","text":"FPN backbones - resnet18 resnet34 resnet50 resnet101 resnet152 resnext50_32x4d resnext101_32x8d wide_resnet50_2 wide_resnet101_2 Resnet backbone - resnet18 resnet34 resnet50 resnet101 resnet152 resnext101_32x8d MobileNet - mobilenet VGG vgg11 vgg13 vgg16 vgg19","title":"Supported Backbones"},{"location":"changing_the_colors/","text":"Changing the colors If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder Color scheme Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) }) Primary color The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accent color The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Changing the colors"},{"location":"changing_the_colors/#changing-the-colors","text":"If you install the documentation on your local machine, you can pick the colors of your choice. The colors can be set from mkdocs.yml located in the docs/ folder","title":"Changing the colors"},{"location":"changing_the_colors/#color-scheme","text":"Our documenation supports two color schemes : a light mode, which is just called default , and a dark mode, which is called slate . The color scheme can be set from mkdocs.yml : theme : palette : scheme : default click on a tile to change the color scheme: default slate var buttons = document.querySelectorAll(\"button[data-md-color-scheme]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-scheme\") document.body.setAttribute(\"data-md-color-scheme\", attr) var name = document.querySelector(\"#__code_0 code span:nth-child(7)\") name.textContent = attr }) })","title":"Color scheme"},{"location":"changing_the_colors/#primary-color","text":"The primary color is used for the header, the sidebar, text links and several other components. In order to change the primary color, set the following value in mkdocs.yml to a valid color name: theme : palette : primary : indigo click on a tile to change the primary color: red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange brown grey blue grey black white var buttons = document.querySelectorAll(\"button[data-md-color-primary]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-primary\") document.body.setAttribute(\"data-md-color-primary\", attr) var name = document.querySelector(\"#__code_2 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) })","title":"Primary color"},{"location":"changing_the_colors/#accent-color","text":"The accent color is used to denote elements that can be interacted with, e.g. hovered links, buttons and scrollbars. It can be changed in mkdocs.yml by chosing a valid color name: theme : palette : accent : indigo click on a tile to change the accent color: .md-typeset button[data-md-color-accent] > code { background-color: var(--md-code-bg-color); color: var(--md-accent-fg-color); } red pink purple deep purple indigo blue light blue cyan teal green light green lime yellow amber orange deep orange var buttons = document.querySelectorAll(\"button[data-md-color-accent]\") buttons.forEach(function(button) { button.addEventListener(\"click\", function() { var attr = this.getAttribute(\"data-md-color-accent\") document.body.setAttribute(\"data-md-color-accent\", attr) var name = document.querySelector(\"#__code_3 code span:nth-child(7)\") name.textContent = attr.replace(\"-\", \" \") }) }) Accessibility \u2013 not all color combinations work well With 2 (color schemes) x 21 (primary colors) x 17 (accent color) = 714 combinations, it's impossible to ensure that all configurations provide a good user experience (e.g. yellow on light background ), so make sure that the color combination of your choosing provides enough contrast and tweak CSS variables where necessary.","title":"Accent color"},{"location":"code_of_conduct/","text":"Contributor Covenant Code of Conduct Our Pledge In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Code of Conduct"},{"location":"code_of_conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"code_of_conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"code_of_conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"code_of_conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"code_of_conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"code_of_conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at airctic@gmail.com. All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"code_of_conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"contributing/","text":"Contribution Guide We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps Step 1: Forking and Installing IceVision \u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, install Poetry by following the instructions here . Then, to locally install the package: poetry install -E all Step 2: Stay in Sync with the original (upstream) repo Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master Step 3: Creating a new branch git checkout -b feature-name git branch master * feature_name: Step 4: Make changes, and commit your file changes Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary. Step 5: Submitting a Pull Request 1. Create a pull request git Upload your local branch to your remote GitHub repo (github.com/username/icevision) cd to/icevision/folder black . git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request 2. Confirm PR was created: Ensure your PR is listed here 3. Updating a PR: Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before> Reviewing Your PR Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well. Resolving Conflicts In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts. Feature Requests and questions For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Contributing Guide"},{"location":"contributing/#contribution-guide","text":"We value your contribution We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. Please, follow these steps","title":"Contribution Guide"},{"location":"contributing/#step-1-forking-and-installing-icevision","text":"\u200b1. Fork the repo to your own github account. click the Fork button to create your own repo copy under your GitHub account. Once forked, you're responsible for keeping your repo copy up-to-date with the upstream icevision repo. \u200b2. Download a copy of your remote username/icevision repo to your local machine. This is the working directory where you will make changes: git clone https://github.com/username/icevision.git cd icevision Install icevision as an editable package. As a best practice, it is highly recommended to create either a mini-conda or a conda environment. Please, check out our Installation Using Conda Guide . First, install Poetry by following the instructions here . Then, to locally install the package: poetry install -E all","title":"Step 1: Forking and Installing IceVision"},{"location":"contributing/#step-2-stay-in-sync-with-the-original-upstream-repo","text":"Set the upstream to sync with this repo. This will keep you in sync with icevision easily. git remote add upstream https://github.com/airctic/icevision.git Updating your local repo: Pull the upstream (original) repo. git checkout master git pull upstream master","title":"Step 2: Stay in Sync with the original (upstream) repo"},{"location":"contributing/#step-3-creating-a-new-branch","text":"git checkout -b feature-name git branch master * feature_name:","title":"Step 3: Creating a new branch"},{"location":"contributing/#step-4-make-changes-and-commit-your-file-changes","text":"Edit files in your favorite editor, and format the code with black # View changes git status # See which files have changed git diff # See changes within files git add path/to/file git commit -m \"Your meaningful commit message for the change.\" Add more commits, if necessary.","title":"Step 4: Make changes, and commit your file changes"},{"location":"contributing/#step-5-submitting-a-pull-request","text":"","title":"Step 5: Submitting a Pull Request"},{"location":"contributing/#1-create-a-pull-request-git","text":"Upload your local branch to your remote GitHub repo (github.com/username/icevision) cd to/icevision/folder black . git push After the push completes, a message may display a URL to automatically submit a pull request to the upstream repo. If not, go to the icevision main repo and GitHub will prompt you to create a pull request. Fill out the Title and the Description of your pull request. Then, click the Submit Pull Request","title":"1. Create a pull request git"},{"location":"contributing/#2-confirm-pr-was-created","text":"Ensure your PR is listed here","title":"2. Confirm PR was created:"},{"location":"contributing/#3-updating-a-pr","text":"Same as before, normally push changes to your branch and the PR will get automatically updated. git commit -m \"updated the feature\" cd to/icevision/folder black . git push origin <enter-branch-name-same-as-before>","title":"3.  Updating a PR:"},{"location":"contributing/#reviewing-your-pr","text":"Maintainers and other contributors will review your pull request. Please participate in the discussion and make the requested changes. When your pull request is approved, it will be merged into the upstream icevision repo. note IceVision has CI checking. It will automatically check your code for build as well.","title":"Reviewing Your PR"},{"location":"contributing/#resolving-conflicts","text":"In your PR, you will see the message like below when the branch is not synced properly or changes were requested. \"This branch has conflicts that must be resolved\" Click Resolve conflicts button near the bottom of your pull request. Then, a file with conflict will be shown with conflict markers <<<<<<< , ======= , and >>>>>>> . <<<<<<< edit-contributor Local Change ======= Remote Change >>>>>>> master The line between <<<<<<< and ======= is your local change and the line between ======= and >>>>>>> is the remote change. Make the changes you want in the final merge. Click Mark as resolved button after you've resolved all the conflicts. You might need to select next file if you have more than one file with a conflict. Click Commit merge button to merge base branch into the head branch. Then click Merge pull request to finish resolving conflicts.","title":"Resolving Conflicts"},{"location":"contributing/#feature-requests-and-questions","text":"For Feature Requests and more questions raise a github issue . We will be happy to assist you. Be sure to check the documentation .","title":"Feature Requests and questions"},{"location":"custom_parser/","text":"Custom Parser - Simple This tutorial uses the Global Wheat Detection dataset, you can download it from Kaggle here . Instaling icevision Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] Imports As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd Understand the data format In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. source = Path ( \"/home/lgvaz/data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser . Create the Parser When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: parsers.FasterRCNN : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. parsers.FilepathMixin : Provides the requirements for parsing images filepaths. parsers.SizeMixin : Provides the requirements for parsing the image dimensions. The first step is to create a class that inherits from these smaller building blocks: class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): pass We now use a method generate_template that will print out all the necessary methods we have to implement. WheatParser . generate_template () def __iter__(self) -> Any: def height(self, o) -> int: def width(self, o) -> int: def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: def imageid(self, o) -> Hashable: With this, we know what methods we have to implement and what each one should return (thanks to the type annotations)! Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ (a single DataFrame row here). Important Be sure to return the correct type on all overriden methods! class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): def __init__ ( self , df , source ): self . df = df self . source = source def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . image_id def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def image_height ( self , o ) -> int : return o . height def image_width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Let's randomly split the data and parser with Parser.parse : parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse () Let's take a look at one record: show_record ( train_rs [ 0 ], label = False ) Conclusion And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that IceVision provides! Happy Learning! If you need any assistance, feel free to join our forum .","title":"Custom Parser"},{"location":"custom_parser/#custom-parser-simple","text":"This tutorial uses the Global Wheat Detection dataset, you can download it from Kaggle here .","title":"Custom Parser - Simple"},{"location":"custom_parser/#instaling-icevision","text":"Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ]","title":"Instaling icevision"},{"location":"custom_parser/#imports","text":"As always, let's import everything from icevision . Additionally, we will also need pandas (you might need to install it with pip install pandas ). from icevision.all import * import pandas as pd","title":"Imports"},{"location":"custom_parser/#understand-the-data-format","text":"In this task we were given a .csv file with annotations, let's take a look at that. Important Replace source with your own path for the dataset directory. source = Path ( \"/home/lgvaz/data/wheat\" ) df = pd . read_csv ( source / \"train.csv\" ) df . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } <div class=\"k-default-codeblock\"> <div class=\"highlight\"><pre><span></span><code>.dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } </code></pre></div> </div> image_id width height bbox source 0 b6ab77fd7 1024 1024 [834.0, 222.0, 56.0, 36.0] usask_1 1 b6ab77fd7 1024 1024 [226.0, 548.0, 130.0, 58.0] usask_1 2 b6ab77fd7 1024 1024 [377.0, 504.0, 74.0, 160.0] usask_1 3 b6ab77fd7 1024 1024 [834.0, 95.0, 109.0, 107.0] usask_1 4 b6ab77fd7 1024 1024 [26.0, 144.0, 124.0, 117.0] usask_1 At first glance, we can make the following assumptions: Multiple rows with the same object_id, width, height A different bbox for each row source doesn't seem relevant right now Once we know what our data provides we can create our custom Parser .","title":"Understand the data format"},{"location":"custom_parser/#create-the-parser","text":"When creating a Parser we inherit from smaller building blocks that provides the functionallity we want: parsers.FasterRCNN : Since we only need to predict bboxes we will use a FasterRCNN model, this will parse all the requirements for using such a model. parsers.FilepathMixin : Provides the requirements for parsing images filepaths. parsers.SizeMixin : Provides the requirements for parsing the image dimensions. The first step is to create a class that inherits from these smaller building blocks: class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): pass We now use a method generate_template that will print out all the necessary methods we have to implement. WheatParser . generate_template () def __iter__(self) -> Any: def height(self, o) -> int: def width(self, o) -> int: def filepath(self, o) -> Union[str, Path]: def bboxes(self, o) -> List[BBox]: def labels(self, o) -> List[int]: def imageid(self, o) -> Hashable: With this, we know what methods we have to implement and what each one should return (thanks to the type annotations)! Defining the __init__ is completely up to you, normally we have to pass our data (the df in our case) and the folder where our images are contained ( source in our case). We then override __iter__ , telling our parser how to iterate over our data. In our case we call df.itertuples to iterate over all df rows. __len__ is not obligatory but will help visualizing the progress when parsing. And finally we override all the other methods, they all receive a single argument o , which is the object returned by __iter__ (a single DataFrame row here). Important Be sure to return the correct type on all overriden methods! class WheatParser ( parsers . FasterRCNN , parsers . FilepathMixin , parsers . SizeMixin ): def __init__ ( self , df , source ): self . df = df self . source = source def __iter__ ( self ): yield from self . df . itertuples () def __len__ ( self ): return len ( self . df ) def imageid ( self , o ) -> Hashable : return o . image_id def filepath ( self , o ) -> Union [ str , Path ]: return self . source / f \" { o . image_id } .jpg\" def image_height ( self , o ) -> int : return o . height def image_width ( self , o ) -> int : return o . width def labels ( self , o ) -> List [ int ]: return [ 1 ] def bboxes ( self , o ) -> List [ BBox ]: return [ BBox . from_xywh ( * np . fromstring ( o . bbox [ 1 : - 1 ], sep = \",\" ))] Let's randomly split the data and parser with Parser.parse : parser = WheatParser ( df , source / \"train\" ) train_rs , valid_rs = parser . parse () Let's take a look at one record: show_record ( train_rs [ 0 ], label = False )","title":"Create the Parser"},{"location":"custom_parser/#conclusion","text":"And that's it! Now that you have your data in the standard library record format, you can use it to create a Dataset , visualize the image with the annotations and basically use all helper functions that IceVision provides!","title":"Conclusion"},{"location":"custom_parser/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"data_splits/","text":"[source] DataSplitter icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source] RandomSplitter icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source] FixedSplitter icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source] SingleSplitSplitter icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"Data Splitters"},{"location":"data_splits/#datasplitter","text":"icevision . data . DataSplitter ( * args , ** kwargs ) Base class for all data splitters. [source]","title":"DataSplitter"},{"location":"data_splits/#randomsplitter","text":"icevision . data . RandomSplitter ( probs , seed = None ) Randomly splits items. Arguments probs Sequence[int] : Sequence of probabilities that must sum to one. The length of the Sequence is the number of groups to to split the items into. seed int : Internal seed used for shuffling the items. Define this if you need reproducible results. Examples Split data into three random groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) data_splitter = RandomSplitter ([ 0.6 , 0.2 , 0.2 ], seed = 42 ) splits = data_splitter ( idmap ) np . testing . assert_equal ( splits , [[ 1 , 3 ], [ 0 ], [ 2 ]]) [source]","title":"RandomSplitter"},{"location":"data_splits/#fixedsplitter","text":"icevision . data . FixedSplitter ( splits ) Split ids based on predefined splits. Arguments: splits: The predefined splits. Examples Split data into three pre-defined groups. idmap = IDMap ([ \"file1\" , \"file2\" , \"file3\" , \"file4\" ]) presplits = [[ \"file4\" , \"file3\" ], [ \"file2\" ], [ \"file1\" ]] data_splitter = FixedSplitter ( presplits ) splits = data_splitter ( idmap = idmap ) assert splits == [[ 3 , 2 ], [ 1 ], [ 0 ]] [source]","title":"FixedSplitter"},{"location":"data_splits/#singlesplitsplitter","text":"icevision . data . SingleSplitSplitter ( * args , ** kwargs ) Return all items in a single group, without shuffling.","title":"SingleSplitSplitter"},{"location":"dataset/","text":"[source] Dataset icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm icevision.tfms.transform.Transform : Transforms to be applied to each item. [source] from_images Dataset . from_images ( images , tfm = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"Dataset"},{"location":"dataset/#dataset","text":"icevision . data . dataset . Dataset ( records , tfm = None ) Container for a list of records and transforms. Steps each time an item is requested (normally via directly indexing the Dataset ): * Grab a record from the internal list of records. * Prepare the record (open the image, open the mask, add metadata). * Apply transforms to the record. Arguments records List[dict] : A list of records. tfm icevision.tfms.transform.Transform : Transforms to be applied to each item. [source]","title":"Dataset"},{"location":"dataset/#from_images","text":"Dataset . from_images ( images , tfm = None ) Creates a Dataset from a list of images. Arguments images Sequence[numpy.array] : Sequence of images in memory (numpy arrays). tfm icevision.tfms.transform.Transform : Transforms to be applied to each item.","title":"from_images"},{"location":"dataset_voc_nb/","text":"How to train a voc compatible dataset. Note: This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating both data, and parsers files as opposed to the fridge dataset example. Installing IceVision Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] Clone the raccoom dataset repository ! git clone https : // github . com / datitran / raccoon_dataset Imports from icevision.all import * WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above Set images and annotations directories data_dir = Path ( 'raccoon_dataset' ) images_dir = data_dir / 'images' annotations_dir = data_dir / 'annotations' Define class_map class_map = ClassMap ([ 'raccoon' ]) Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Datasets Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) Fastai Learner metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Fastai Training Learning Rate Finder learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612) Fine tune: 2 Phases Phase 1: Train the head for 10 epochs while freezing the body Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 ) Show results efficientdet . show_results ( model , valid_ds , class_map = class_map ) Note: You might train the model longer in order to increase its accuracy Happy Learning! If you need any assistance, feel free to join our forum .","title":"Training a VOC dataset"},{"location":"dataset_voc_nb/#how-to-train-a-voc-compatible-dataset","text":"Note: This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating both data, and parsers files as opposed to the fridge dataset example.","title":"How to train a voc compatible dataset."},{"location":"dataset_voc_nb/#installing-icevision","text":"Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ]","title":"Installing IceVision"},{"location":"dataset_voc_nb/#clone-the-raccoom-dataset-repository","text":"! git clone https : // github . com / datitran / raccoon_dataset","title":"Clone the raccoom dataset repository"},{"location":"dataset_voc_nb/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"dataset_voc_nb/#warning","text":"Make sure you have already cloned the raccoon dataset using the command shown here above","title":"WARNING:"},{"location":"dataset_voc_nb/#set-images-and-annotations-directories","text":"data_dir = Path ( 'raccoon_dataset' ) images_dir = data_dir / 'images' annotations_dir = data_dir / 'annotations'","title":"Set images and annotations directories"},{"location":"dataset_voc_nb/#define-class_map","text":"class_map = ClassMap ([ 'raccoon' ])","title":"Define class_map"},{"location":"dataset_voc_nb/#parser-use-icevision-predefined-voc-parser","text":"parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map )","title":"Parser: Use icevision predefined VOC parser"},{"location":"dataset_voc_nb/#train-and-validation-records","text":"train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"train and validation records"},{"location":"dataset_voc_nb/#datasets","text":"","title":"Datasets"},{"location":"dataset_voc_nb/#transforms","text":"presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"dataset_voc_nb/#train-and-validation-dataset-objects","text":"train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Train and Validation Dataset Objects"},{"location":"dataset_voc_nb/#dataloaders","text":"train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoaders"},{"location":"dataset_voc_nb/#model","text":"model = efficientdet . model ( 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size )","title":"Model"},{"location":"dataset_voc_nb/#fastai-learner","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Fastai Learner"},{"location":"dataset_voc_nb/#fastai-training","text":"","title":"Fastai Training"},{"location":"dataset_voc_nb/#learning-rate-finder","text":"learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612)","title":"Learning Rate Finder"},{"location":"dataset_voc_nb/#fine-tune-2-phases","text":"Phase 1: Train the head for 10 epochs while freezing the body Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 )","title":"Fine tune: 2 Phases"},{"location":"dataset_voc_nb/#show-results","text":"efficientdet . show_results ( model , valid_ds , class_map = class_map ) Note: You might train the model longer in order to increase its accuracy","title":"Show results"},{"location":"dataset_voc_nb/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"deployment/","text":"Deployment We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Overview"},{"location":"deployment/#deployment","text":"We offer some easy-to-use options to deploy models trained using IceVision framework. Please, check out the deployment section in our documentation or the icevision-gradio repository. We are using gradio because it is a powerful yet to easy-to-use deployment option.","title":"Deployment"},{"location":"efficientdet/","text":"[source] model icevision . models . efficientdet . model . model ( model_name , num_classes , img_size , pretrained = True ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments model_name str : Specifies the model to create. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 64. pretrained bool : If True, use a pretrained backbone (on COCO). Returns A PyTorch model. [source] train_dl icevision . models . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . efficientdet . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . efficientdet . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . efficientdet . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. # Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"efficientdet/#model","text":"icevision . models . efficientdet . model . model ( model_name , num_classes , img_size , pretrained = True ) Creates the efficientdet model specified by model_name . The model implementation is by Ross Wightman, original repo here . Arguments model_name str : Specifies the model to create. For pretrained models, check this table. num_classes int : Number of classes of your dataset (including background). img_size int : Image size that will be fed to the model. Must be squared and divisible by 64. pretrained bool : If True, use a pretrained backbone (on COCO). Returns A PyTorch model. [source]","title":"model"},{"location":"efficientdet/#train_dl","text":"icevision . models . efficientdet . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"efficientdet/#valid_dl","text":"icevision . models . efficientdet . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"efficientdet/#infer_dl","text":"icevision . models . efficientdet . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"efficientdet/#build_train_batch","text":"icevision . models . efficientdet . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"efficientdet/#build_valid_batch","text":"icevision . models . efficientdet . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"efficientdet/#build_infer_batch","text":"icevision . models . efficientdet . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. # Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"efficientdet_fastai/","text":"[source] learner icevision . models . efficientdet . fastai . learner . learner ( dls , model , param_groups = None , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"efficientdet_fastai/#learner","text":"icevision . models . efficientdet . fastai . learner . learner ( dls , model , param_groups = None , cbs = None , ** learner_kwargs ) Fastai Learner adapted for EfficientDet. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"efficientdet_lightning/","text":"[source] ModelAdapter icevision . models . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"efficientdet_lightning/#modeladapter","text":"icevision . models . efficientdet . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for EfficientDet, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics List[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"efficientdet_pets/","text":"How to use EfficientDet Installing IceVision Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata Imports from icevision.all import * Common part to all models Loading Data data_dir = icedata . pets . load () Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Datasets presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) EffecientDet Specific Part DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size ) Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Fastai Training learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Inference DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) imgs = [ sample [ 'img' ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"EffecientDet"},{"location":"efficientdet_pets/#how-to-use-efficientdet","text":"","title":"How to use EfficientDet"},{"location":"efficientdet_pets/#installing-icevision","text":"Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata","title":"Installing IceVision"},{"location":"efficientdet_pets/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"efficientdet_pets/#common-part-to-all-models","text":"","title":"Common part to all models"},{"location":"efficientdet_pets/#loading-data","text":"data_dir = icedata . pets . load ()","title":"Loading Data"},{"location":"efficientdet_pets/#parser","text":"class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Parser"},{"location":"efficientdet_pets/#datasets","text":"presize = 512 size = 384 train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet )","title":"Datasets"},{"location":"efficientdet_pets/#effecientdet-specific-part","text":"","title":"EffecientDet Specific Part"},{"location":"efficientdet_pets/#dataloaders","text":"train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoaders"},{"location":"efficientdet_pets/#model","text":"model = efficientdet . model ( model_name = 'tf_efficientdet_lite0' , num_classes = len ( class_map ), img_size = size )","title":"Model"},{"location":"efficientdet_pets/#fastai-learner","text":"metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Fastai Learner"},{"location":"efficientdet_pets/#fastai-training","text":"learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.012022644281387329, lr_steep=0.10000000149011612) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Training"},{"location":"efficientdet_pets/#inference","text":"","title":"Inference"},{"location":"efficientdet_pets/#dataloader","text":"infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 )","title":"DataLoader"},{"location":"efficientdet_pets/#predict","text":"samples , preds = efficientdet . predict_dl ( model , infer_dl ) imgs = [ sample [ 'img' ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Predict"},{"location":"efficientdet_pets/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"faster_rcnn/","text":"[source] model icevision . models . rcnn . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source] train_dl icevision . models . rcnn . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . rcnn . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . rcnn . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . rcnn . faster_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . rcnn . faster_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . rcnn . faster_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"faster_rcnn/#model","text":"icevision . models . rcnn . faster_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** faster_rcnn_kwargs ) FasterRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **faster_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.faster_rcnn.FastRCNN . Returns A Pytorch nn.Module . [source]","title":"model"},{"location":"faster_rcnn/#train_dl","text":"icevision . models . rcnn . faster_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"faster_rcnn/#valid_dl","text":"icevision . models . rcnn . faster_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"faster_rcnn/#infer_dl","text":"icevision . models . rcnn . faster_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"faster_rcnn/#build_train_batch","text":"icevision . models . rcnn . faster_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records Sequence[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"faster_rcnn/#build_valid_batch","text":"icevision . models . rcnn . faster_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"faster_rcnn/#build_infer_batch","text":"icevision . models . rcnn . faster_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"faster_rcnn_fastai/","text":"[source] learner icevision . models . rcnn . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"faster_rcnn_fastai/#learner","text":"icevision . models . rcnn . faster_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Faster RCNN. Arguments dls Sequence[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs Optional[Sequence[fastai.callback.core.Callback]] : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"faster_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . rcnn . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"faster_rcnn_lightning/#modeladapter","text":"icevision . models . rcnn . faster_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"getting_started/","text":"Getting started with IceVision Why IceVision? IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly Introduction This tutorial walks you through the different steps of training and using a model. The IceVision Framework is an agnostic framework . To demonstrate this we will train and use our model with both the fastai , and pytorch-lightning libraries. If you are using Google Colab, the GPU runtime should be enabled, but if you experience problems when training your model, you may want to check this. Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU Install icevision and icedata ! pip install icevision [ all ] ! pip install icedata Import the package from icevision.all import * import icedata Datasets IceVision provides handy methods to load a dataset, parse annotations, and more. In the example below, we work with the PETS dataset to detect cats and dogs in images and identify their species. Loading the PETS dataset is one line code. data_dir = icedata . pets . load_data () data_dir Parser The Parser is one of the most important concepts in IceVision. It allows us to work with any annotation format. The basic job of the parser is to convert a custom format to something the library can understand. You might still need to create a custom parser for your own dataset. Fear not! Creating parsers is easy. After you've finished this tutorial, check this custom parser documentation to understand how to. IceVision already provides a parser for the Pets Dataset class_map = icedata . pets . class_map () class_map parser = icedata . pets . parser ( data_dir , class_map ) Parse the data Next we parse() the dataset using the data splitter. This returns returns 2 lists of records: one for training and another for validation. Behind the scenes we shuffle the data and proceed with a 80% train 20% valid split. train_records , valid_records = parser . parse () What's a record? A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotation has, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.) Visualize the training data We can show one of the records (image + box + label). This helps to understand what is in the dataset and check that the boxes and labels make sense. show_record ( train_records [ 1 ]) We can also display the label instead of its identifier by providing the class_map . show_record ( train_records [ 1 ], class_map = class_map ) Of course, we often want to see several images with their corresponding boxes and labels. records = train_records [: 6 ] show_records ( records , ncols = 3 , class_map = class_map ) Transforms Data transformations are an essential part of the training pipeline. There are many transformation libraries available including: albumentations , solt , and torchvision . IceVision supports the widely used albumentations library out-of-the-box. It is possible to integrate other transform libraries. You just need to inherit and override all abstract methods of the Transform class. We plan to add more to future versions in response to community feedback. It is typical to use different transformations for the training and validation datasets. The valid_tfms apply to the validation set. These are minimal - just resizing the image and normalising it. The train_tfms typically do data augmentations such as zoom, crop, lighting adjustments, horizontal flips, and so on. These help to reduce the required training set size, reduce overfitting, and produce a more robust model. Icevision makes this easy - all of the bounding boxes are adjusted if needed. For example, zooming in will make the bounding boxes larger. Crops will not cut any bounding boxes. The presize parameter helps to improve the resulting image quality. See the Fast AI Book for more details. The A.Normalize function applies a set of default normalizations that have been refined over the years on the Imagenet dataset. presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()]) Dataset The Dataset class combines the records and transforms. To create a Dataset , we just need need to pass the parsed records from the previous step along with the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What does the Dataset class do? Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. Applies the pipeline of transforms to the record prepared in the previous step Lazy transforms Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. Important Because we normalized our images with imagenet_stats , when displaying transformed images, we need to denormalize them. The show_sample function receives an optional argument called denormalize_fn that we can be passed: In our case, we pass denormalize_imagenet . Displaying the same image with different transforms samples = [ train_ds [ 3 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , class_map = class_map ) Model In this tutorial, we are learning to predict bounding boxes and classes, but not performing image segmentation. We will use the FasterRCNN model. To create the model, we need to specify how many classes our dataset has. This is the length of the class_map . Note that the class_map includes a value for \"background\" with index 0, which is added behind the scenes by default. model = faster_rcnn . model ( num_classes = len ( class_map )) DataLoader Each model has its own dataloader (a pytorch DataLoader ) that could be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Training IceVision is an agnostic framework meaning it can be plugged to multiple DL frameworks such as fastai , and pytorch-lightning . You could also plug it into a new DL frameworks using your own custom code. Metrics Metrics are essential for tracking the model progress as it's training. Here we are going to be using the well established COCOMetric , which reports on the mean average precision of the predictions. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training with fastai Creating a Learner object Creating a fastai compatible Learner using the fastai interface. learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) Training the RCNN model using fastai fine_tune() method The fastai fine_tune method is useful when you have a pre-trained model, which we are using. It does an initial epoch where it freezes everything except its final layers. It then carries on for the indicated number of epochs using a differential learning rate to train the whole model. It adjusts the learning rate both across the layers of the model as well as across the epochs. This can give excellent results with reduced training time. In September 2020, if everything is working, the model might require around 3 minutes per epoch on a free Google Colab server. learn . fine_tune ( 10 , lr = 1e-4 ) Training with Pytorch-Lightning Creating a Pytorch-Lightning (PL) model class It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) **Note:** If you are familiar to working with lightning, you will note that we've been able to skip some of the boilerplate. This is because the IceVision `RCNNLightningAdapter` takes care of it behind the scene. For example, it defines `training_step` and `validation_step`. The adaptor also supports working with `Metric`s. If you need custom functionality, you can override or re-implement those methods. light_model = LightModel ( model , metrics = metrics ) Training the RCNN model using PL Trainer.fit() method trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Visualize results To quickly visualize the results of the model on a specific dataset use show_results : faster_rcnn . show_results ( model , valid_ds , class_map = class_map ) Inference Load a model Training the model with fastai using fine_tune twice and I got led the the following results: train_loss: 0.06772 valid_loss: 0.074435 Using our Trained Weights If you don't want to train the model, you can use our trained weights that we publicly available: You can download them with torch.hub : weights_url = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cpu\" )) Note Typically inference is done on the cpu, this is why we specify the paramater map_location to cpu when loading the state dict. Let's recreate the model and load the downloaded weights: model = faster_rcnn . model ( num_classes = len ( class_map )) model . load_state_dict ( state_dict ) model . cuda () The first step for prediction is to have some images, let's grab some random ones from the validation dataset: 11.3- Predict all images at once If you don't have too many images, you can get predictions with a single forward pass. In case your images don't fit in memory simultaneously, you should predict in batches, the next section shows how to do that. For demonstration purposes, let's take download a single image from the internet and see how our model performs on it. IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img ) <AxesSubplot:> Try other images! Change IMAGE_URL to point to another image you found on the internet. Just be sure to take one of the breeds from class_map , or else the model might get confused. Whenever you have images in memory (numpy arrays) you can use Dataset.from_images . We're going to use the same transforms we used on the validation dataset. infer_ds = Dataset . from_images ([ img ], valid_tfms ) For any model, the prediction steps are always the same, first call build_infer_batch and then predict . For faster_rcnn we have detection_threshold , which specifies how confident the model should be to output a bounding box. batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) For displaying the predictions, we first need to grab our image from samples . We do this instead of using the original images because transforms may have been applied to the image (in fact, in this case, a resize was used). imgs = [ sample [ \"img\" ] for sample in samples ] Now we just need to call show_preds , to show the image with its corresponding predictions (boxes + labels). show_preds ( imgs = imgs , preds = preds , class_map = class_map , show = True ) 11.4- Predicting a batch of images Instead of predicting a whole list of images at one, we can process a small batch at the time: This option is more memory efficient: We use infer_dataloader Had we have a test dataset, we would have maken our predicition using the batch technique mentionned here above. As an illustrative example, we will predict all images belonging to the validation dataset using the following approach: infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Same as before, we grab our images from samples . imgs = [ sample [ \"img\" ] for sample in samples ] Let's show the first 6 predictions: show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], ncols = 3 , class_map = class_map , show = True , ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Getting Started"},{"location":"getting_started/#getting-started-with-icevision","text":"","title":"Getting started with IceVision"},{"location":"getting_started/#why-icevision","text":"IceVision is an Object-Detection Framework that connects to different libraries/frameworks such as fastai, Pytorch Lightning, and Pytorch with more to come. Features a Unified Data API with out-of-the-box support for common annotation formats (COCO, VOC, etc.) The IceData repo hosts community maintained parsers and custom datasets Provides flexible model implementations with pluggable backbones Helps researchers reproduce, replicate, and go beyond published models Enables practioners to get moving with object detection technology quickly","title":"Why IceVision?"},{"location":"getting_started/#introduction","text":"This tutorial walks you through the different steps of training and using a model. The IceVision Framework is an agnostic framework . To demonstrate this we will train and use our model with both the fastai , and pytorch-lightning libraries. If you are using Google Colab, the GPU runtime should be enabled, but if you experience problems when training your model, you may want to check this. Runtime -> Change runtime type -> Hardware accelerator dropdown -> GPU","title":"Introduction"},{"location":"getting_started/#install-icevision-and-icedata","text":"! pip install icevision [ all ] ! pip install icedata","title":"Install icevision and icedata"},{"location":"getting_started/#import-the-package","text":"from icevision.all import * import icedata","title":"Import the package"},{"location":"getting_started/#datasets","text":"IceVision provides handy methods to load a dataset, parse annotations, and more. In the example below, we work with the PETS dataset to detect cats and dogs in images and identify their species. Loading the PETS dataset is one line code. data_dir = icedata . pets . load_data () data_dir","title":"Datasets"},{"location":"getting_started/#parser","text":"The Parser is one of the most important concepts in IceVision. It allows us to work with any annotation format. The basic job of the parser is to convert a custom format to something the library can understand. You might still need to create a custom parser for your own dataset. Fear not! Creating parsers is easy. After you've finished this tutorial, check this custom parser documentation to understand how to. IceVision already provides a parser for the Pets Dataset class_map = icedata . pets . class_map () class_map parser = icedata . pets . parser ( data_dir , class_map )","title":"Parser"},{"location":"getting_started/#parse-the-data","text":"Next we parse() the dataset using the data splitter. This returns returns 2 lists of records: one for training and another for validation. Behind the scenes we shuffle the data and proceed with a 80% train 20% valid split. train_records , valid_records = parser . parse () What's a record? A record is a dictionary that contains all parsed fields defined by the parser used. No matter what format the annotation has, a record has a common structure that can be connected to different DL frameworks (fastai, Pytorch-Lightning, etc.)","title":"Parse the data"},{"location":"getting_started/#visualize-the-training-data","text":"We can show one of the records (image + box + label). This helps to understand what is in the dataset and check that the boxes and labels make sense. show_record ( train_records [ 1 ]) We can also display the label instead of its identifier by providing the class_map . show_record ( train_records [ 1 ], class_map = class_map ) Of course, we often want to see several images with their corresponding boxes and labels. records = train_records [: 6 ] show_records ( records , ncols = 3 , class_map = class_map )","title":"Visualize the training data"},{"location":"getting_started/#transforms","text":"Data transformations are an essential part of the training pipeline. There are many transformation libraries available including: albumentations , solt , and torchvision . IceVision supports the widely used albumentations library out-of-the-box. It is possible to integrate other transform libraries. You just need to inherit and override all abstract methods of the Transform class. We plan to add more to future versions in response to community feedback. It is typical to use different transformations for the training and validation datasets. The valid_tfms apply to the validation set. These are minimal - just resizing the image and normalising it. The train_tfms typically do data augmentations such as zoom, crop, lighting adjustments, horizontal flips, and so on. These help to reduce the required training set size, reduce overfitting, and produce a more robust model. Icevision makes this easy - all of the bounding boxes are adjusted if needed. For example, zooming in will make the bounding boxes larger. Crops will not cut any bounding boxes. The presize parameter helps to improve the resulting image quality. See the Fast AI Book for more details. The A.Normalize function applies a set of default normalizations that have been refined over the years on the Imagenet dataset. presize = 512 size = 384 valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"getting_started/#dataset","text":"The Dataset class combines the records and transforms. To create a Dataset , we just need need to pass the parsed records from the previous step along with the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) What does the Dataset class do? Prepares the record: For example, in the record we just have a filename that points to the image, it's at this stage that we open the image. Applies the pipeline of transforms to the record prepared in the previous step Lazy transforms Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. Important Because we normalized our images with imagenet_stats , when displaying transformed images, we need to denormalize them. The show_sample function receives an optional argument called denormalize_fn that we can be passed: In our case, we pass denormalize_imagenet .","title":"Dataset"},{"location":"getting_started/#displaying-the-same-image-with-different-transforms","text":"samples = [ train_ds [ 3 ] for _ in range ( 6 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"getting_started/#model","text":"In this tutorial, we are learning to predict bounding boxes and classes, but not performing image segmentation. We will use the FasterRCNN model. To create the model, we need to specify how many classes our dataset has. This is the length of the class_map . Note that the class_map includes a value for \"background\" with index 0, which is added behind the scenes by default. model = faster_rcnn . model ( num_classes = len ( class_map ))","title":"Model"},{"location":"getting_started/#dataloader","text":"Each model has its own dataloader (a pytorch DataLoader ) that could be customized: the dataloaders for the RCNN models have a custom collate function. train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False )","title":"DataLoader"},{"location":"getting_started/#training","text":"IceVision is an agnostic framework meaning it can be plugged to multiple DL frameworks such as fastai , and pytorch-lightning . You could also plug it into a new DL frameworks using your own custom code.","title":"Training"},{"location":"getting_started/#metrics","text":"Metrics are essential for tracking the model progress as it's training. Here we are going to be using the well established COCOMetric , which reports on the mean average precision of the predictions. metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"getting_started/#training-with-fastai","text":"","title":"Training with fastai"},{"location":"getting_started/#creating-a-learner-object","text":"Creating a fastai compatible Learner using the fastai interface. learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics )","title":"Creating a Learner object"},{"location":"getting_started/#training-the-rcnn-model-using-fastai-fine_tune-method","text":"The fastai fine_tune method is useful when you have a pre-trained model, which we are using. It does an initial epoch where it freezes everything except its final layers. It then carries on for the indicated number of epochs using a differential learning rate to train the whole model. It adjusts the learning rate both across the layers of the model as well as across the epochs. This can give excellent results with reduced training time. In September 2020, if everything is working, the model might require around 3 minutes per epoch on a free Google Colab server. learn . fine_tune ( 10 , lr = 1e-4 )","title":"Training the RCNN model using fastai fine_tune() method"},{"location":"getting_started/#training-with-pytorch-lightning","text":"","title":"Training with Pytorch-Lightning"},{"location":"getting_started/#creating-a-pytorch-lightning-pl-model-class","text":"It inherits from RCNNLightningAdapter and implements the method PL configure_optimizers . class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) **Note:** If you are familiar to working with lightning, you will note that we've been able to skip some of the boilerplate. This is because the IceVision `RCNNLightningAdapter` takes care of it behind the scene. For example, it defines `training_step` and `validation_step`. The adaptor also supports working with `Metric`s. If you need custom functionality, you can override or re-implement those methods. light_model = LightModel ( model , metrics = metrics )","title":"Creating a Pytorch-Lightning (PL) model class"},{"location":"getting_started/#training-the-rcnn-model-using-pl-trainerfit-method","text":"trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training the RCNN model using PL Trainer.fit() method"},{"location":"getting_started/#visualize-results","text":"To quickly visualize the results of the model on a specific dataset use show_results : faster_rcnn . show_results ( model , valid_ds , class_map = class_map )","title":"Visualize results"},{"location":"getting_started/#inference","text":"","title":"Inference"},{"location":"getting_started/#load-a-model","text":"Training the model with fastai using fine_tune twice and I got led the the following results: train_loss: 0.06772 valid_loss: 0.074435","title":"Load a model"},{"location":"getting_started/#using-our-trained-weights","text":"If you don't want to train the model, you can use our trained weights that we publicly available: You can download them with torch.hub : weights_url = \"https://github.com/airctic/model_zoo/releases/download/m3/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( weights_url , map_location = torch . device ( \"cpu\" )) Note Typically inference is done on the cpu, this is why we specify the paramater map_location to cpu when loading the state dict. Let's recreate the model and load the downloaded weights: model = faster_rcnn . model ( num_classes = len ( class_map )) model . load_state_dict ( state_dict ) model . cuda () The first step for prediction is to have some images, let's grab some random ones from the validation dataset:","title":"Using our Trained Weights"},{"location":"getting_started/#113-predict-all-images-at-once","text":"If you don't have too many images, you can get predictions with a single forward pass. In case your images don't fit in memory simultaneously, you should predict in batches, the next section shows how to do that. For demonstration purposes, let's take download a single image from the internet and see how our model performs on it. IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img ) <AxesSubplot:> Try other images! Change IMAGE_URL to point to another image you found on the internet. Just be sure to take one of the breeds from class_map , or else the model might get confused. Whenever you have images in memory (numpy arrays) you can use Dataset.from_images . We're going to use the same transforms we used on the validation dataset. infer_ds = Dataset . from_images ([ img ], valid_tfms ) For any model, the prediction steps are always the same, first call build_infer_batch and then predict . For faster_rcnn we have detection_threshold , which specifies how confident the model should be to output a bounding box. batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) For displaying the predictions, we first need to grab our image from samples . We do this instead of using the original images because transforms may have been applied to the image (in fact, in this case, a resize was used). imgs = [ sample [ \"img\" ] for sample in samples ] Now we just need to call show_preds , to show the image with its corresponding predictions (boxes + labels). show_preds ( imgs = imgs , preds = preds , class_map = class_map , show = True )","title":"11.3- Predict all images at once"},{"location":"getting_started/#114-predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process a small batch at the time: This option is more memory efficient: We use infer_dataloader Had we have a test dataset, we would have maken our predicition using the batch technique mentionned here above. As an illustrative example, we will predict all images belonging to the validation dataset using the following approach: infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Same as before, we grab our images from samples . imgs = [ sample [ \"img\" ] for sample in samples ] Let's show the first 6 predictions: show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], ncols = 3 , class_map = class_map , show = True , )","title":"11.4- Predicting a batch of images"},{"location":"getting_started/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"how-to/","text":"Where can I get some help? If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum . How to install icevision? To install the IceVision package as well as all its dependencies, choose one of the 2 options: Installing the IceVision lastest version pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade Install the IceVision lastest version from Pypi repository: pip install icevision [ all ] For more options, and more in-depth explanation on how to install IceVision, please check out our Installation Guide How to create an EffecientDet Model? tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Model as well as the EffecientDet Backbone documents. How to create a Faster RCNN Model? fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents/ How to create a Mask RCNN Model? - Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents. How to use EffecientDet Backbones? EffecientDet backbones are passed as string argument to the effecientdet model function: model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Backbone document. How to use Faster RCNN Backbones? Faster RCNN backbones are passed a model object argument to the Faster RCNN model function: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document. How to use Mask RCNN Backbones? Mask RCNN backbones are passed a model object argument to the Mask RCNN model function: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document. How to predict (infer) a single image? This is a quick example using the PETS dataset: # Imports from icevision.all import * # Maps from IDs to class names. `print(class_map)` for all available classes class_map = datasets . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained on `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , ) How to save trained weights in Google Colab? In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by IceVision Check out the Train a Dataset Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' ) How to load pretrained weights? In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) How to contribute? We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How-To"},{"location":"how-to/#where-can-i-get-some-help","text":"If you find a bug, or you would like to suggest some new features, please file an issue here If you need any assistance during your learning journey, feel free to join our forum .","title":"Where can I get some help?"},{"location":"how-to/#how-to-install-icevision","text":"To install the IceVision package as well as all its dependencies, choose one of the 2 options: Installing the IceVision lastest version pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade Install the IceVision lastest version from Pypi repository: pip install icevision [ all ] For more options, and more in-depth explanation on how to install IceVision, please check out our Installation Guide","title":"How to install icevision?"},{"location":"how-to/#how-to-create-an-effecientdet-model","text":"tf_efficientdet_lite0 Example: Source Code model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) efficientdet_d0 Example: model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Model as well as the EffecientDet Backbone documents.","title":"How to create an EffecientDet Model?"},{"location":"how-to/#how-to-create-a-faster-rcnn-model","text":"fasterrcnn_resnet50_fpn Example: Source Code - Using the default argument model = faster_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents/","title":"How to create a Faster RCNN Model?"},{"location":"how-to/#how-to-create-a-mask-rcnn-model","text":"- Using the default argument model = mask_rcnn . model ( num_classes = len ( class_map )) Using the explicit backbone definition backbone = backbones . resnet_fpn . resnet50 ( pretrained = True ) # Default model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Model as well as the Faster RCNN Backbone documents.","title":"How to create a Mask RCNN Model?"},{"location":"how-to/#how-to-use-effecientdet-backbones","text":"EffecientDet backbones are passed as string argument to the effecientdet model function: model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) For more information checkout the EffecientDet Backbone document.","title":"How to use EffecientDet Backbones?"},{"location":"how-to/#how-to-use-faster-rcnn-backbones","text":"Faster RCNN backbones are passed a model object argument to the Faster RCNN model function: backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document.","title":"How to use Faster RCNN Backbones?"},{"location":"how-to/#how-to-use-mask-rcnn-backbones","text":"Mask RCNN backbones are passed a model object argument to the Mask RCNN model function: backbone = backbones . resnet_fpn . resnet34 ( pretrained = True ) model = mask_rcnn . model ( backbone = backbone , num_classes = len ( class_map ) ) For more information checkout the Faster RCNN Backbone document.","title":"How to use Mask RCNN Backbones?"},{"location":"how-to/#how-to-predict-infer-a-single-image","text":"This is a quick example using the PETS dataset: # Imports from icevision.all import * # Maps from IDs to class names. `print(class_map)` for all available classes class_map = datasets . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained on `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"How to predict (infer) a single image?"},{"location":"how-to/#how-to-save-trained-weights-in-google-colab","text":"In the following example, we show how to save trained weight using an EffecientDet model. The latter can be replaced by any model supported by IceVision Check out the Train a Dataset Notebook to get familiar with all the steps from the training a dataset to saving the trained weights. # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Train the model using either Fastai Learner of Pytorch-Lightning Trainer ## Saving a Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' )","title":"How to save trained weights in Google Colab?"},{"location":"how-to/#how-to-load-pretrained-weights","text":"In this example, we show how to create a Faster RCNN model, and load pretrained weight that were previously obtained during the training of the PETS dataset as shown in the Getting Started Notebook # Maps IDs to class names. class_map = datasets . pets . class_map () # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict )","title":"How to load pretrained weights?"},{"location":"how-to/#how-to-contribute","text":"We are both a welcoming and an open community. We warmly invite you to join us either as a user or a community contributor. We will be happy to hear from you. To contribute, please follow the Contributing Guide .","title":"How to contribute?"},{"location":"inference/","text":"Use a trained model for inference Install IceVision We usually install IceVision with [all] , but you can also use [inference] to only install the dependencies necessary for inference. Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata Imports As always, import everything from IceVision: from icevision.all import * Loading the model We're going to use the model trained on the getting started tutorial. Saving/Loading a model Save your model with torch.save(model.state_dict(), PATH) . Take a look at this tutorial for more info on how to save/load models. The first thing we need is the ClassMap used during training. The model we're going to use was trained on the Pets dataset, so we can easily grab that: class_map = icedata . pets . class_map () Recreate the model used in training: model = faster_rcnn . model ( num_classes = len ( class_map )) And now load the model weights (commonly refered as state_dict in Pytorch). In our case, the weights are stored in the cloud, Pytorch is amazing and provides us with a function to directly load model weights from an URL. Where to host your model You can save your model directly on github via a \"release\" Simply go to https://github.com/<ACCOUNT>/<REPO>/releases/new and upload the model as a new release. This will generate a direct link for downloading the model. WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" )) If your weights are stored locally, you simply need to do: state_dict = torch.load(<PATH>) Model device Notice that we're asking to load the weights on CPU with map_location . It's common to do inference in CPU, but you can easily change to GPU if you want. Now, let's actually load the weights to the model: model . load_state_dict ( state_dict ) <All keys matched successfully> Transforms Generally speaking, we normally use on inference the same transforms we used in the validation set. A transform like normalize is always required, but for some models, a transform like resize is optional. For instance, the model used in this tutorial accepts any image resolution, try playing around with different transforms and observe how they change the model results. Let's use the same transforms used in the validation set: infer_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 384 ), tfms . A . Normalize ()]) Data Obviously we need some images to perform inference, how you get this images heavily depends on your use case. For this tutorial, let's grab an image from an URL: import PIL , requests def image_from_url ( url ): res = requests . get ( url , stream = True ) img = PIL . Image . open ( res . raw ) return np . array ( img ) image_url = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" img = image_from_url ( image_url ) show_img ( img ); Try it out Try experimenting with new images, be sure to take one of the breeds from class_map (or not, if you are curious to see what happens). Whenever we have images in memory (numpy arrays) we can use Dataset.from_images to easily create a Dataset from it: infer_ds = Dataset . from_images ([ img ], infer_tfms ) Predict - All at once First build a batch with the entire dataset, and then simply call predict : batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) Predict - In batches If the memory is not enough to predict everything at once, break it down into smaller batches with infer_dataloader : infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) Visualize This step will probably not be the same for your use case, but for quickly visualizing the predictions we can use show_preds : # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Inference"},{"location":"inference/#use-a-trained-model-for-inference","text":"","title":"Use a trained model for inference"},{"location":"inference/#install-icevision","text":"We usually install IceVision with [all] , but you can also use [inference] to only install the dependencies necessary for inference. Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata","title":"Install IceVision"},{"location":"inference/#imports","text":"As always, import everything from IceVision: from icevision.all import *","title":"Imports"},{"location":"inference/#loading-the-model","text":"We're going to use the model trained on the getting started tutorial. Saving/Loading a model Save your model with torch.save(model.state_dict(), PATH) . Take a look at this tutorial for more info on how to save/load models. The first thing we need is the ClassMap used during training. The model we're going to use was trained on the Pets dataset, so we can easily grab that: class_map = icedata . pets . class_map () Recreate the model used in training: model = faster_rcnn . model ( num_classes = len ( class_map )) And now load the model weights (commonly refered as state_dict in Pytorch). In our case, the weights are stored in the cloud, Pytorch is amazing and provides us with a function to directly load model weights from an URL. Where to host your model You can save your model directly on github via a \"release\" Simply go to https://github.com/<ACCOUNT>/<REPO>/releases/new and upload the model as a new release. This will generate a direct link for downloading the model. WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" )) If your weights are stored locally, you simply need to do: state_dict = torch.load(<PATH>) Model device Notice that we're asking to load the weights on CPU with map_location . It's common to do inference in CPU, but you can easily change to GPU if you want. Now, let's actually load the weights to the model: model . load_state_dict ( state_dict ) <All keys matched successfully>","title":"Loading the model"},{"location":"inference/#transforms","text":"Generally speaking, we normally use on inference the same transforms we used in the validation set. A transform like normalize is always required, but for some models, a transform like resize is optional. For instance, the model used in this tutorial accepts any image resolution, try playing around with different transforms and observe how they change the model results. Let's use the same transforms used in the validation set: infer_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 384 ), tfms . A . Normalize ()])","title":"Transforms"},{"location":"inference/#data","text":"Obviously we need some images to perform inference, how you get this images heavily depends on your use case. For this tutorial, let's grab an image from an URL: import PIL , requests def image_from_url ( url ): res = requests . get ( url , stream = True ) img = PIL . Image . open ( res . raw ) return np . array ( img ) image_url = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" img = image_from_url ( image_url ) show_img ( img ); Try it out Try experimenting with new images, be sure to take one of the breeds from class_map (or not, if you are curious to see what happens). Whenever we have images in memory (numpy arrays) we can use Dataset.from_images to easily create a Dataset from it: infer_ds = Dataset . from_images ([ img ], infer_tfms )","title":"Data"},{"location":"inference/#predict-all-at-once","text":"First build a batch with the entire dataset, and then simply call predict : batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch )","title":"Predict - All at once"},{"location":"inference/#predict-in-batches","text":"If the memory is not enough to predict everything at once, break it down into smaller batches with infer_dataloader : infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl )","title":"Predict - In batches"},{"location":"inference/#visualize","text":"This step will probably not be the same for your use case, but for quickly visualizing the predictions we can use show_preds : # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"Visualize"},{"location":"inference/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"install/","text":"Important We currently only support Linux/MacOS installations A- Installation using pip Option 1: Installing from pypi repository [Stable Version] To install icevision package together with all dependencies: $ pip install icevision [ all ] Option 2: Installing an editable package locally [For Developers] Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e \".[all,dev]\" Option 3: Installing a non-editable package from GitHub: To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade B- Installation using conda Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called ice $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ] C- Fixing the Error: Failed building wheel for pycocotools If you encounter the Failed building wheel for pycocotools error (see screenshoot here below), you can easily fix it by installing gcc from your linux terminal as shown in the following steps: $ sudo apt update $ sudo apt install gcc Note You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient installation option for you.","title":"Installation"},{"location":"install/#a-installation-using-pip","text":"","title":"A- Installation using pip"},{"location":"install/#option-1-installing-from-pypi-repository-stable-version","text":"To install icevision package together with all dependencies: $ pip install icevision [ all ]","title":"Option 1: Installing from pypi repository [Stable Version]"},{"location":"install/#option-2-installing-an-editable-package-locally-for-developers","text":"Note This method is used by developers who are usually either: actively contributing to icevision project by adding new features or fixing bugs, or creating their own extensions, and making sure that their source code stay in sync with the icevision latest version. Then, clone the repo and install the package: $ git clone --depth = 1 https://github.com/airctic/icevision.git $ cd icevision $ pip install -e \".[all,dev]\"","title":"Option 2: Installing an editable package locally [For Developers]"},{"location":"install/#option-3-installing-a-non-editable-package-from-github","text":"To install the icevision package from its GitHub repo, run the command here below. This option can be used in Google Colab, for example, where you might install the icevision latest version (from the master branch) $ pip install git+git://github.com/airctic/icevision.git#egg = icevision [ all ] --upgrade","title":"Option 3: Installing a non-editable package from GitHub:"},{"location":"install/#b-installation-using-conda","text":"Creating a conda environment is considered as a best practice because it avoids polluting the default (base) environment, and reduces dependencies conflicts. Use the following command in order to create a conda environment called ice $ conda create -n icevision python = 3 .8 anaconda $ conda activate icevision $ pip install icevision [ all ]","title":"B- Installation using conda"},{"location":"install/#c-fixing-the-error-failed-building-wheel-for-pycocotools","text":"If you encounter the Failed building wheel for pycocotools error (see screenshoot here below), you can easily fix it by installing gcc from your linux terminal as shown in the following steps: $ sudo apt update $ sudo apt install gcc Note You can check out the following blog post: 3 ways to pip install a package for more a detailed explantion on how to choose the most convenient installation option for you.","title":"C- Fixing the Error: Failed building wheel for pycocotools"},{"location":"mask_rcnn/","text":"[source] model icevision . models . rcnn . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source] train_dl icevision . models . rcnn . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] valid_dl icevision . models . rcnn . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] infer_dl icevision . models . rcnn . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source] build_train_batch icevision . models . rcnn . mask_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source] build_valid_batch icevision . models . rcnn . mask_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source] build_infer_batch icevision . models . rcnn . mask_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"common"},{"location":"mask_rcnn/#model","text":"icevision . models . rcnn . mask_rcnn . model . model ( num_classes , backbone = None , remove_internal_transforms = True , ** mask_rcnn_kwargs ) MaskRCNN model implemented by torchvision. Arguments num_classes int : Number of classes. backbone Optional[torch.nn.modules.module.Module] : Backbone model to use. Defaults to a resnet50_fpn model. remove_internal_transforms bool : The torchvision model internally applies transforms like resizing and normalization, but we already do this at the Dataset level, so it's safe to remove those internal transforms. **mask_rcnn_kwargs : Keyword arguments that internally are going to be passed to torchvision.models.detection.mask_rcnn.MaskRCNN . Return A Pytorch nn.Module . [source]","title":"model"},{"location":"mask_rcnn/#train_dl","text":"icevision . models . rcnn . mask_rcnn . dataloaders . train_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for training the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"train_dl"},{"location":"mask_rcnn/#valid_dl","text":"icevision . models . rcnn . mask_rcnn . dataloaders . valid_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for validating the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"valid_dl"},{"location":"mask_rcnn/#infer_dl","text":"icevision . models . rcnn . mask_rcnn . dataloaders . infer_dl ( dataset , batch_tfms = None , ** dataloader_kwargs ) A DataLoader with a custom collate_fn that batches items as required for inferring the model. Arguments dataset : Possibly a Dataset object, but more generally, any Sequence that returns records. batch_tfms : Transforms to be applied at the batch level. **dataloader_kwargs : Keyword arguments that will be internally passed to a Pytorch DataLoader . The parameter collate_fn is already defined internally and cannot be passed here. Returns A Pytorch DataLoader . [source]","title":"infer_dl"},{"location":"mask_rcnn/#build_train_batch","text":"icevision . models . rcnn . mask_rcnn . dataloaders . build_train_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when training. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_train_batch ( records ) outs = model ( * batch ) [source]","title":"build_train_batch"},{"location":"mask_rcnn/#build_valid_batch","text":"icevision . models . rcnn . mask_rcnn . dataloaders . build_valid_batch ( records , batch_tfms = None ) Builds a batch in the format required by the model when validating. Arguments records List[Dict[str, Any]] : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_valid_batch ( records ) outs = model ( * batch ) [source]","title":"build_valid_batch"},{"location":"mask_rcnn/#build_infer_batch","text":"icevision . models . rcnn . mask_rcnn . dataloaders . build_infer_batch ( dataset , batch_tfms = None ) Builds a batch in the format required by the model when doing inference. Arguments records : A Sequence of records. batch_tfms : Transforms to be applied at the batch level. Returns A tuple with two items. The first will be a tuple like (images, targets) , in the input format required by the model. The second will be an updated list of the input records with batch_tfms applied. Examples Use the result of this function to feed the model. batch , records = build_infer_batch ( records ) outs = model ( * batch )","title":"build_infer_batch"},{"location":"mask_rcnn_fastai/","text":"[source] learner icevision . models . rcnn . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"fastai"},{"location":"mask_rcnn_fastai/#learner","text":"icevision . models . rcnn . mask_rcnn . fastai . learner . learner ( dls , model , cbs = None , ** learner_kwargs ) Fastai Learner adapted for Mask RCNN. Arguments dls List[Union[torch.utils.data.dataloader.DataLoader, fastai.data.load.DataLoader]] : Sequence of DataLoaders passed to the Learner . The first one will be used for training and the second for validation. model torch.nn.modules.module.Module : The model to train. cbs : Optional Sequence of callbacks. **learner_kwargs : Keyword arguments that will be internally passed to Learner . Returns A fastai Learner .","title":"learner"},{"location":"mask_rcnn_lightning/","text":"[source] ModelAdapter icevision . models . rcnn . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"lightning"},{"location":"mask_rcnn_lightning/#modeladapter","text":"icevision . models . rcnn . mask_rcnn . lightning . model_adapter . ModelAdapter ( model , metrics = None ) Lightning module specialized for faster_rcnn, with metrics support. The methods forward , training_step , validation_step , validation_epoch_end are already overriden. Arguments model torch.nn.modules.module.Module : The pytorch model to use. metrics Sequence[icevision.metrics.metric.Metric] : Sequence of metrics to use. Returns A LightningModule .","title":"ModelAdapter"},{"location":"mask_rcnn_pennfudan/","text":"How to use Mask RCNN Installing IceVision We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata Imports from icevision.all import * Data We'll be using the Penn-Fundan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(FloatProgress(value=0.0, max=170.0), HTML(value=''))) Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) Metrics Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)] Model Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map )) Training - fastai We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) Visualize predictions Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Mask RCNN"},{"location":"mask_rcnn_pennfudan/#how-to-use-mask-rcnn","text":"","title":"How to use Mask RCNN"},{"location":"mask_rcnn_pennfudan/#installing-icevision","text":"We ussually install IceVision with [all] , but we can also use [inference] to install only the packages that inference methods depend on. Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata","title":"Installing IceVision"},{"location":"mask_rcnn_pennfudan/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"mask_rcnn_pennfudan/#data","text":"We'll be using the Penn-Fundan dataset, which is already available under datasets . data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () As usual, let's create the parser and perfom a random data split. parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () HBox(children=(FloatProgress(value=0.0, max=170.0), HTML(value=''))) Let's use the usual aug_tfms for training transforms with two small modifications: - Decrease the rotation limit from 45 to 10. - Use a more aggresive crop function. shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( 384 // 2 , 384 ), p =. 5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn ), tfms . A . Normalize (), ] ) And for validation transforms, the simple resize_and_pad . valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = 348 ), tfms . A . Normalize ()]) Now we can create the Dataset and take a look on how the images look after the transforms. train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 1 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , display_label = False , show = True ) Now we're ready to create the DataLoaders: train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 )","title":"Data"},{"location":"mask_rcnn_pennfudan/#metrics","text":"Metrics are a work in progress for Mask RCNN. # metrics = [COCOMetric(COCOMetricType.mask)]","title":"Metrics"},{"location":"mask_rcnn_pennfudan/#model","text":"Similarly to faster_rcnn , we just need the num_classes to create a Mask RCNN model. model = mask_rcnn . model ( num_classes = len ( class_map ))","title":"Model"},{"location":"mask_rcnn_pennfudan/#training-fastai","text":"We just need to create the learner and fine-tune. Optional You can use learn.lr_find() for finding a good learning rate. learn = mask_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 )","title":"Training - fastai"},{"location":"mask_rcnn_pennfudan/#visualize-predictions","text":"Let's grab some images from valid_ds to visualize. For more info on how to do inference, check the inference tutorial . mask_rcnn . show_results ( model , valid_ds , class_map = class_map )","title":"Visualize predictions"},{"location":"mask_rcnn_pennfudan/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"model_comparison/","text":"Models Source IceVision offers both EffecientDet and Faster RCNN models. IceVision uses a unified API that makes it easy for the users to swap one model by another as it is shown in the following example. Click one or the other tab to compare both implementations and discover the strong similarities between the two implementations: EffecientDet # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Faster RCNN # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Faster RCNN Specific Part # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = faster_rcnn . model ( num_classes = len ( class_map )) # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Models Comparison"},{"location":"model_comparison/#models","text":"Source IceVision offers both EffecientDet and Faster RCNN models. IceVision uses a unified API that makes it easy for the users to swap one model by another as it is shown in the following example. Click one or the other tab to compare both implementations and discover the strong similarities between the two implementations: EffecientDet # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Faster RCNN # Installing IceVision # !pip install git+git://github.com/airctic/icevision.git#egg=icevision[all] --upgrade # Imports from icevision.all import * # Common part to all models # Loading Data data_dir = datasets . pets . load () # Parser class_map = datasets . pets . class_map () parser = datasets . pets . parser ( data_dir , class_map ) data_splitter = RandomSplitter ([ 0.8 , 0.2 ]) train_records , valid_records = parser . parse ( data_splitter ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Faster RCNN Specific Part # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model model = faster_rcnn . model ( num_classes = len ( class_map )) # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Models"},{"location":"model_efficientdet/","text":"EffecientDet Model Source EffecientDet is one of the effecient and fastest object detection model that also uses more constrained resources in comparison to other models (Fig. 1). It was introduced in the following paper: EfficientDet: Scalable and Efficient Object Detection Usage To train an EffecientDet model, you need to call the following highlighted functions shown here below. Common part to both Fastai and Pytorch-Lightning Training Loop DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates an EffecientDet model implemented by Ross Wightman . The model accepts a variety of backbones. In following example, the tf_efficientdet_lite0 is used. We can also choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) How to use a different backbone: \"efficientdet_d0\" example # Model model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size ) Fastai Example Once the DataLoaders and the EffecientDet model are created, we create the fastai Learner. The latter uses the DataLoaders and the EffecientDet model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the EffecientDet model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Pytorch-Lightning Example The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same EffecientDet model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the EffecientDet model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Train using pytorch-lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) How to train the PETS Dataset using EffecientDet Source Code Background: Paper Abstract Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detectors. References Paper EfficientDet: Scalable and Efficient Object Detection Implementation We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. Currently this includes all models implemented by the EficientNet and MobileNetv3 classes (which also includes MNasNet, MobileNetV2, MixNet and more)","title":"EffecientDet"},{"location":"model_efficientdet/#effecientdet-model","text":"Source EffecientDet is one of the effecient and fastest object detection model that also uses more constrained resources in comparison to other models (Fig. 1). It was introduced in the following paper: EfficientDet: Scalable and Efficient Object Detection","title":"EffecientDet Model"},{"location":"model_efficientdet/#usage","text":"To train an EffecientDet model, you need to call the following highlighted functions shown here below.","title":"Usage"},{"location":"model_efficientdet/#common-part-to-both-fastai-and-pytorch-lightning-training-loop","text":"DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates an EffecientDet model implemented by Ross Wightman . The model accepts a variety of backbones. In following example, the tf_efficientdet_lite0 is used. We can also choose one of the efficientdet_d0 to efficientdet_d7 backbones, and MobileNetv3 classes (which also includes MNasNet , MobileNetV2 , MixNet and more) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) How to use a different backbone: \"efficientdet_d0\" example # Model model = efficientdet . model ( model_name = \"efficientdet_d0\" , num_classes = len ( class_map ), img_size = size )","title":"Common part to both Fastai and Pytorch-Lightning Training Loop"},{"location":"model_efficientdet/#fastai-example","text":"Once the DataLoaders and the EffecientDet model are created, we create the fastai Learner. The latter uses the DataLoaders and the EffecientDet model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the EffecientDet model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Example"},{"location":"model_efficientdet/#pytorch-lightning-example","text":"The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same EffecientDet model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the EffecientDet model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the EffecientDet model. # Train using pytorch-lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Pytorch-Lightning Example"},{"location":"model_efficientdet/#how-to-train-the-pets-dataset-using-effecientdet","text":"Source Code","title":"How to train the PETS Dataset using EffecientDet"},{"location":"model_efficientdet/#background-paper-abstract","text":"Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multiscale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and better backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with singlemodel and single-scale, our EfficientDet-D7 achieves stateof-the-art 55.1 AP on COCO test-dev with 77M parameters and 410B FLOPs1, being 4x \u2013 9x smaller and using 13x \u2013 42x fewer FLOPs than previous detectors.","title":"Background: Paper Abstract"},{"location":"model_efficientdet/#references","text":"","title":"References"},{"location":"model_efficientdet/#paper","text":"EfficientDet: Scalable and Efficient Object Detection","title":"Paper"},{"location":"model_efficientdet/#implementation","text":"We use Ross Wightman's implementation which is an accurate port of the official TensorFlow (TF) implementation that accurately preserves the TF training weights EfficientDet (PyTorch) Any backbone in the timm model collection that supports feature extraction (features_only arg) can be used as a bacbkone. Currently this includes all models implemented by the EficientNet and MobileNetv3 classes (which also includes MNasNet, MobileNetV2, MixNet and more)","title":"Implementation"},{"location":"model_faster_rcnn/","text":"Faster RCNN Model Source Faster RCNN is one of the most popular object detection model. It was introduced in the following paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Usage To train a Faster RCNN model, you need to call the following highlighted functions shown here below. Common part to both Fastai and Pytorch-Lightning Training Loop DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates a Faster RCNN model implemented in torchvision FasterRCNN . The model accepts a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the following backbones : resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 # Model model = faster_rcnn . model ( num_classes = len ( class_map )) How to use a different backbone: \"resnet18\" example # Backbone backbone = faster_rcnn . backbones . resnet_fpn . resnet18 ( pretrained = True ) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) Fastai Example Once the DataLoaders and the Faster RCNN model are created, we create the fastai Learner. The latter uses the DataLoaders and the Faster RCNN model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the Faster RCNN model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) Pytorch-Lightning Example The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same Faster RCNN model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the Faster RCNN model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) How to train the PETS Dataset using Faster RCNN Source Code Paper Introduction Faster R-CNN is built upon the knowledge of Fast RCNN which indeed built upon the ideas of RCNN and SPP-Net. In their paper, the authors introduced a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. References Paper Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks Blog Posts An overview of deep-learning based object-detection algorithms Guide to build Faster RCNN in PyTorch Torchvision Implementation Torchvision Object Detection Finetuning Tutorial Torchvision Faster RCNN Implementation","title":"Faster RCNN"},{"location":"model_faster_rcnn/#faster-rcnn-model","text":"Source Faster RCNN is one of the most popular object detection model. It was introduced in the following paper: Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks","title":"Faster RCNN Model"},{"location":"model_faster_rcnn/#usage","text":"To train a Faster RCNN model, you need to call the following highlighted functions shown here below.","title":"Usage"},{"location":"model_faster_rcnn/#common-part-to-both-fastai-and-pytorch-lightning-training-loop","text":"DataLoaders: Manstisshrimp creates common train DataLoader and valid DataLoader to both fastai Learner and Pytorch-Lightning Trainer # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) Model: IceVision creates a Faster RCNN model implemented in torchvision FasterRCNN . The model accepts a variety of backbones. In following example, we use the default fasterrcnn_resnet50_fpn model. We can also choose one of the following backbones : resnet18, resnet34, resnet50, resnet101, resnet152, resnext50_32x4d, resnext101_32x8d, wide_resnet50_2, wide_resnet101_2 # Model model = faster_rcnn . model ( num_classes = len ( class_map )) How to use a different backbone: \"resnet18\" example # Backbone backbone = faster_rcnn . backbones . resnet_fpn . resnet18 ( pretrained = True ) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map ))","title":"Common part to both Fastai and Pytorch-Lightning Training Loop"},{"location":"model_faster_rcnn/#fastai-example","text":"Once the DataLoaders and the Faster RCNN model are created, we create the fastai Learner. The latter uses the DataLoaders and the Faster RCNN model shared with the Pytorch-Lightning Trainer (as shown in the Pytorch-Lightning example here below): Fastai Learner: It glues the Faster RCNN model with the DataLoaders as well as the metrics and any other fastai Learner arguments. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Fastai Learner metrics = [ COCOMetric ()] learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 )","title":"Fastai Example"},{"location":"model_faster_rcnn/#pytorch-lightning-example","text":"The Pytorch-Lightning example is quiet similar to the fastai one in a sense it uses the same DataLoaders objects, and the same Faster RCNN model. Those are subsenquently passed on to the Pytorch-Lightning Trainer: Pytorch-Lightning Trainer: It glues the Faster RCNN model with the DataLoaders. In Pytorch-Lightning, the metrics are passed to the model object as opposed to fastai where it is passed to the Learner object. In the code snippet shown here below, we highlight the parts related to the Faster RCNN model. # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Pytorch-Lightning Example"},{"location":"model_faster_rcnn/#how-to-train-the-pets-dataset-using-faster-rcnn","text":"Source Code","title":"How to train the PETS Dataset using Faster RCNN"},{"location":"model_faster_rcnn/#paper-introduction","text":"Faster R-CNN is built upon the knowledge of Fast RCNN which indeed built upon the ideas of RCNN and SPP-Net. In their paper, the authors introduced a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position.","title":"Paper Introduction"},{"location":"model_faster_rcnn/#references","text":"","title":"References"},{"location":"model_faster_rcnn/#paper","text":"Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks","title":"Paper"},{"location":"model_faster_rcnn/#blog-posts","text":"An overview of deep-learning based object-detection algorithms Guide to build Faster RCNN in PyTorch","title":"Blog Posts"},{"location":"model_faster_rcnn/#torchvision-implementation","text":"Torchvision Object Detection Finetuning Tutorial Torchvision Faster RCNN Implementation","title":"Torchvision Implementation"},{"location":"parser/","text":"[source] Parser icevision . parsers . Parser ( * args , ** kwargs ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required field for all parsers is the image_id . Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source] parse Parser . parse ( data_splitter = None , idmap = None , autofix = True , show_pbar = True ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. idmap icevision.core.id_map.IDMap : Maps from filenames to unique ids, pass an IDMap() if you need this information. show_pbar bool : Whether or not to show a progress bar while parsing the data. Returns A list of records for each split defined by data_splitter . [source] FasterRCNN icevision . parsers . FasterRCNN ( * args , ** kwargs ) Parser with required mixins for Faster RCNN. [source] MaskRCNN icevision . parsers . MaskRCNN ( * args , ** kwargs ) Parser with required mixins for Mask RCNN. [source] ImageidMixin icevision . parsers . mixins . ImageidMixin ( * args , ** kwargs ) Adds imageid method to parser [source] FilepathMixin icevision . parsers . mixins . FilepathMixin ( * args , ** kwargs ) Adds filepath method to parser [source] SizeMixin icevision . parsers . mixins . SizeMixin ( * args , ** kwargs ) Adds image_height and image_width method to parser [source] LabelsMixin icevision . parsers . mixins . LabelsMixin ( * args , ** kwargs ) Adds labels method to parser [source] BBoxesMixin icevision . parsers . mixins . BBoxesMixin ( * args , ** kwargs ) Adds bboxes method to parser [source] MasksMixin icevision . parsers . mixins . MasksMixin ( * args , ** kwargs ) Adds masks method to parser [source] AreasMixin icevision . parsers . mixins . AreasMixin ( * args , ** kwargs ) Adds areas method to parser [source] IsCrowdsMixin icevision . parsers . mixins . IsCrowdsMixin ( * args , ** kwargs ) Adds iscrowds method to parser","title":"Parser"},{"location":"parser/#parser","text":"icevision . parsers . Parser ( * args , ** kwargs ) Base class for all parsers, implements the main parsing logic. The actual fields to be parsed are defined by the mixins used when defining a custom parser. The only required field for all parsers is the image_id . Examples Create a parser for image filepaths. class FilepathParser ( Parser , FilepathParserMixin ): # implement required abstract methods [source]","title":"Parser"},{"location":"parser/#parse","text":"Parser . parse ( data_splitter = None , idmap = None , autofix = True , show_pbar = True ) Loops through all data points parsing the required fields. Arguments data_splitter icevision.data.DataSplitter : How to split the parsed data, defaults to a [0.8, 0.2] random split. idmap icevision.core.id_map.IDMap : Maps from filenames to unique ids, pass an IDMap() if you need this information. show_pbar bool : Whether or not to show a progress bar while parsing the data. Returns A list of records for each split defined by data_splitter . [source]","title":"parse"},{"location":"parser/#fasterrcnn","text":"icevision . parsers . FasterRCNN ( * args , ** kwargs ) Parser with required mixins for Faster RCNN. [source]","title":"FasterRCNN"},{"location":"parser/#maskrcnn","text":"icevision . parsers . MaskRCNN ( * args , ** kwargs ) Parser with required mixins for Mask RCNN. [source]","title":"MaskRCNN"},{"location":"parser/#imageidmixin","text":"icevision . parsers . mixins . ImageidMixin ( * args , ** kwargs ) Adds imageid method to parser [source]","title":"ImageidMixin"},{"location":"parser/#filepathmixin","text":"icevision . parsers . mixins . FilepathMixin ( * args , ** kwargs ) Adds filepath method to parser [source]","title":"FilepathMixin"},{"location":"parser/#sizemixin","text":"icevision . parsers . mixins . SizeMixin ( * args , ** kwargs ) Adds image_height and image_width method to parser [source]","title":"SizeMixin"},{"location":"parser/#labelsmixin","text":"icevision . parsers . mixins . LabelsMixin ( * args , ** kwargs ) Adds labels method to parser [source]","title":"LabelsMixin"},{"location":"parser/#bboxesmixin","text":"icevision . parsers . mixins . BBoxesMixin ( * args , ** kwargs ) Adds bboxes method to parser [source]","title":"BBoxesMixin"},{"location":"parser/#masksmixin","text":"icevision . parsers . mixins . MasksMixin ( * args , ** kwargs ) Adds masks method to parser [source]","title":"MasksMixin"},{"location":"parser/#areasmixin","text":"icevision . parsers . mixins . AreasMixin ( * args , ** kwargs ) Adds areas method to parser [source]","title":"AreasMixin"},{"location":"parser/#iscrowdsmixin","text":"icevision . parsers . mixins . IsCrowdsMixin ( * args , ** kwargs ) Adds iscrowds method to parser","title":"IsCrowdsMixin"},{"location":"quickstart/","text":"Quickstart Introduction This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata. Installing IceVision and IceData Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata Imports from icevision.all import * Datasets : Fridge Objects dataset Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () Visualization Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) Train and Validation Dataset Transforms # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) Displaying the same image with different transforms Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map ) DataLoader # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 ) Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = 384 ) Metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] Training IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code. Training using fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.014454397559165954, lr_steep=0.12022644281387329) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 ) Training using Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 50 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) Inference Predicting a batch of images Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model , infer_dl ) HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value=''))) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Saving Model on Google Drive from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' ) Happy Learning! If you need any assistance, feel free to join our forum .","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"quickstart/#introduction","text":"This tutorial walk you through the different steps of training the fridge dataset. the IceVision Framework is an agnostic framework . As an illustration, we will train our model using both the fastai library, and pytorch-lightning libraries. For more information about how the fridge dataset as well as its corresponding parser check out the fridge folder in icedata.","title":"Introduction"},{"location":"quickstart/#installing-icevision-and-icedata","text":"Google Colab Dependencies Incompatibilities This issue is specific to Google Colab. The issue shouldn't occur on a local machine. Some of our external dependencies are not aligned with the dependencies pre-installed in Google Colab. After pip installing both icevision and icedata (by runnning the cell here below), some errors will eventually pop up. To fix this issue, press the RESTART RUNTIME button. ! pip install icevision [ all ] icedata","title":"Installing IceVision and IceData"},{"location":"quickstart/#imports","text":"from icevision.all import *","title":"Imports"},{"location":"quickstart/#datasets-fridge-objects-dataset","text":"Fridge Objects dataset is tiny dataset that contains 134 images of 4 classes: - can, - carton, - milk bottle, - water bottle. IceVision provides very handy methods such as loading a dataset, parsing annotations, and more. # Loading Data url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" data_dir = icedata . load_data ( url , dest_dir , force_download = True ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/annotations\" , images_dir = data_dir / \"odFridgeObjects/images\" , class_map = class_map ) # Records train_records , valid_records = parser . parse ()","title":"Datasets : Fridge Objects dataset"},{"location":"quickstart/#visualization","text":"Showing a batch of images with their corresponding boxes and labels show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map )","title":"Visualization"},{"location":"quickstart/#train-and-validation-dataset-transforms","text":"# Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms )","title":"Train and Validation Dataset Transforms"},{"location":"quickstart/#displaying-the-same-image-with-different-transforms","text":"Note: Transforms are applied lazily , meaning they are only applied when we grab (get) an item. This means that, if you have augmentation (random) transforms, each time you get the same item from the dataset you will get a slightly different version of it. samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map )","title":"Displaying the same image with different transforms"},{"location":"quickstart/#dataloader","text":"# DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 )","title":"DataLoader"},{"location":"quickstart/#model","text":"model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = 384 )","title":"Model"},{"location":"quickstart/#metrics","text":"metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )]","title":"Metrics"},{"location":"quickstart/#training","text":"IceVision is an agnostic framework meaning it can be plugged to other DL framework such as fastai2 , and pytorch-lightning . You could also plug to oth DL framework using your own custom code.","title":"Training"},{"location":"quickstart/#training-using-fastai","text":"learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . freeze () learn . lr_find () SuggestedLRs(lr_min=0.014454397559165954, lr_steep=0.12022644281387329) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 )","title":"Training using fastai"},{"location":"quickstart/#training-using-lightning","text":"class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 50 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training using Lightning"},{"location":"quickstart/#inference","text":"","title":"Inference"},{"location":"quickstart/#predicting-a-batch-of-images","text":"Instead of predicting a whole list of images at one, we can process small batch at the time: This option is more memory efficient. infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) samples , preds = efficientdet . predict_dl ( model , infer_dl ) HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value=''))) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Predicting a batch of images"},{"location":"quickstart/#saving-model-on-google-drive","text":"from google.colab import drive drive . mount ( '/content/gdrive' , force_remount = True ) root_dir = Path ( '/content/gdrive/My Drive/' ) torch . save ( model . state_dict (), root_dir / 'icevision/models/fridge/fridge_tf_efficientdet_lite0.pth' )","title":"Saving Model on Google Drive"},{"location":"quickstart/#happy-learning","text":"If you need any assistance, feel free to join our forum .","title":"Happy Learning!"},{"location":"readme_mkdocs/","text":"IceVision Documentation The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs . Building the documentation Locally install the package as described here From the root directory, cd into the docs/ folder and run: poetry run python autogen.py poetry run mkdocs serve # Starts a local webserver: localhost:8000","title":"Generating Docs"},{"location":"readme_mkdocs/#icevision-documentation","text":"The source for IceVision documentation is in the docs/ folder. Our documentation uses extended Markdown, as implemented by MkDocs .","title":"IceVision Documentation"},{"location":"readme_mkdocs/#building-the-documentation","text":"Locally install the package as described here From the root directory, cd into the docs/ folder and run: poetry run python autogen.py poetry run mkdocs serve # Starts a local webserver: localhost:8000","title":"Building the documentation"},{"location":"examples/backbones_faster_rcnn/","text":"Using different Faster RCNN backbones In this example, we are training the Raccoon dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # fastai Learner learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Train using fastai fine tuning learn . fine_tune ( 20 , lr = 1e-4 ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Backbones - Faster RCNN"},{"location":"examples/backbones_faster_rcnn/#using-different-faster-rcnn-backbones","text":"In this example, we are training the Raccoon dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # fastai Learner learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Train using fastai fine tuning learn . fine_tune ( 20 , lr = 1e-4 ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # DataLoaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Backbones backbone = backbones . resnet_fpn . resnet18 ( pretrained = True ) # backbone = backbones.resnet_fpn.resnet34(pretrained=True) # backbone = backbones.resnet_fpn.resnet50(pretrained=True) # Default # backbone = backbones.resnet_fpn.resnet101(pretrained=True) # backbone = backbones.resnet_fpn.resnet152(pretrained=True) # backbone = backbones.resnet_fpn.resnext50_32x4d(pretrained=True) # backbone = backbones.resnet_fpn.resnext101_32x8d(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet50_2(pretrained=True) # backbone = backbones.resnet_fpn.wide_resnet101_2(pretrained=True) # Model model = faster_rcnn . model ( backbone = backbone , num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 20 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = faster_rcnn . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = faster_rcnn . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Using different Faster RCNN backbones"},{"location":"examples/dataset_voc_exp/","text":"How to train a VOC compatible dataset This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating data, and parsers files as opposed to the fridge dataset example. Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Fine tune: 2 Phases # Phase 1: Train the head for 10 epochs while freezing the body # Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 60 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Training a VOC dataset"},{"location":"examples/dataset_voc_exp/#how-to-train-a-voc-compatible-dataset","text":"This notebook shows a special use case of training a VOC compatible dataset using the predefined VOC parser without creating data, and parsers files as opposed to the fridge dataset example. Fastai # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training # Learning Rate Finder learn . freeze () learn . lr_find () # Fine tune: 2 Phases # Phase 1: Train the head for 10 epochs while freezing the body # Phase 2: Train both the body and the head during 50 epochs learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 10 ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Clone the raccoom dataset repository # !git clone https://github.com/datitran/raccoon_dataset # Imports from icevision.all import * # WARNING: Make sure you have already cloned the raccoon dataset using the command shown here above # Set images and annotations directories data_dir = Path ( \"raccoon_dataset\" ) images_dir = data_dir / \"images\" annotations_dir = data_dir / \"annotations\" # Define class_map class_map = ClassMap ([ \"raccoon\" ]) # Parser: Use icevision predefined VOC parser parser = parsers . voc ( annotations_dir = annotations_dir , images_dir = images_dir , class_map = class_map ) # train and validation records train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) # Train and Validation Dataset Objects train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Show some image samples samples = [ train_ds [ 5 ] for _ in range ( 3 )] show_samples ( samples , class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner metrics = [ COCOMetric ()] learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 60 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 16 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show some samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"How to train a VOC compatible dataset"},{"location":"examples/efficientdet_pets_exp/","text":"How to use EffecientDet In this example, we show how to train an EffecientDet model on the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training learn . freeze () learn . lr_find () learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"EffecientDet"},{"location":"examples/efficientdet_pets_exp/#how-to-use-effecientdet","text":"In this example, we show how to train an EffecientDet model on the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Fastai Learner learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) # Fastai Training learn . freeze () learn . lr_find () learn . fine_tune ( 10 , 1e-2 , freeze_epochs = 1 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Common part to all models # Loading Data data_dir = icedata . pets . load_data () # Parser class_map = icedata . pets . class_map () parser = icedata . pets . parser ( data_dir , class_map ) train_records , valid_records = parser . parse () show_records ( train_records [: 3 ], ncols = 3 , class_map = class_map ) # Datasets # Transforms presize = 512 size = 384 train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) samples = [ train_ds [ 0 ] for _ in range ( 3 )] show_samples ( samples , ncols = 3 , class_map = class_map , denormalize_fn = denormalize_imagenet ) # EffecientDet Specific Part # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) batch , samples = first ( train_dl ) show_samples ( samples [: 6 ], class_map = class_map , ncols = 3 , denormalize_fn = denormalize_imagenet ) # Model model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"How to use EffecientDet"},{"location":"examples/getting_started_exp/","text":"Training and End-to-End dataset (PETS) In this example, we are training the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using fastai2 learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , lr = 1e-4 ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse () # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Getting Started"},{"location":"examples/getting_started_exp/#training-and-end-to-end-dataset-pets","text":"In this example, we are training the PETS dataset using either Fastai or Pytorch-Lightning training loop Fastai # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse ( data_splitter ) # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using fastai2 learn = faster_rcnn . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 10 , lr = 1e-4 ) Pytorch Lightning # Installing IceVision # !pip install icevision[all] # Imports from icevision.all import * import icedata # Load the PETS dataset path = icedata . pets . load_data () # Get the class_map, a utility that maps from number IDs to classs names class_map = icedata . pets . class_map () # PETS parser: provided out-of-the-box parser = icedata . pets . parser ( data_dir = path , class_map = class_map ) train_records , valid_records = parser . parse () # shows images with corresponding labels and boxes show_records ( train_records [: 6 ], ncols = 3 , class_map = class_map , show = True ) # Define transforms - using Albumentations transforms out of the box train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size ), tfms . A . Normalize ()]) # Create both training and validation datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Create both training and validation dataloaders train_dl = faster_rcnn . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = faster_rcnn . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Create model model = faster_rcnn . model ( num_classes = len ( class_map )) # Define metrics metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl )","title":"Training and End-to-End dataset (PETS)"},{"location":"examples/inference_exp/","text":"How to use the inference API The inference API is unified one. It is independent from both Fastai or Pytorch-Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Maps IDs to class names. `print(class_map)` for all available classes class_map = icedata . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"Inference"},{"location":"examples/inference_exp/#how-to-use-the-inference-api","text":"The inference API is unified one. It is independent from both Fastai or Pytorch-Lightning # Installing IceVision # !pip install icevision[all] icedata # Imports from icevision.all import * import icedata # Maps IDs to class names. `print(class_map)` for all available classes class_map = icedata . pets . class_map () # Try experimenting with new images, be sure to take one of the breeds from `class_map` IMAGE_URL = \"https://petcaramelo.com/wp-content/uploads/2018/06/beagle-cachorro.jpg\" IMG_PATH = \"tmp.jpg\" # Model trained in `Tutorials->Getting Started` WEIGHTS_URL = \"https://github.com/airctic/model_zoo/releases/download/pets_faster_resnet50fpn/pets_faster_resnetfpn50.zip\" # Download and open image, optionally show it download_url ( IMAGE_URL , IMG_PATH ) img = open_img ( IMG_PATH ) show_img ( img , show = True ) # The model was trained with normalized images, it's necessary to do the same in inference tfms = tfms . A . Adapter ([ tfms . A . Normalize ()]) # Whenever you have images in memory (numpy arrays) you can use `Dataset.from_images` infer_ds = Dataset . from_images ([ img ], tfms ) # Create the same model used in training and load the weights # `map_location` will put the model on cpu, optionally move to gpu if necessary model = faster_rcnn . model ( num_classes = len ( class_map )) state_dict = torch . hub . load_state_dict_from_url ( WEIGHTS_URL , map_location = torch . device ( \"cpu\" ) ) model . load_state_dict ( state_dict ) # For any model, the prediction steps are always the same # First call `build_infer_batch` and then `predict` batch , samples = faster_rcnn . build_infer_batch ( infer_ds ) preds = faster_rcnn . predict ( model = model , batch = batch ) # If instead you want to predict in smaller batches, use `infer_dataloader` infer_dl = faster_rcnn . infer_dl ( infer_ds , batch_size = 1 ) samples , preds = faster_rcnn . predict_dl ( model = model , infer_dl = infer_dl ) # Show preds by grabbing the images from `samples` imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , class_map = class_map , denormalize_fn = denormalize_imagenet , show = True , )","title":"How to use the inference API"},{"location":"examples/mask_rcnn_pennfudan_exp/","text":"Using Mask RCNN This shows how to train a MaskRCNN model on the Penn-Fundan dataset using either Fastai or Pytorch-Lightning training loop. Fastai # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) # Parse records with random splits train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Create Fastai Learner and train the model learn = mask_rcnn . fastai . learner ( model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 ) Pytorch Lightning # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Mask RCNN"},{"location":"examples/mask_rcnn_pennfudan_exp/#using-mask-rcnn","text":"This shows how to train a MaskRCNN model on the Penn-Fundan dataset using either Fastai or Pytorch-Lightning training loop. Fastai # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) # Parse records with random splits train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Create Fastai Learner and train the model learn = mask_rcnn . fastai . learner ( model = model ) learn . fine_tune ( 10 , 5e-4 , freeze_epochs = 2 ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 ) Pytorch Lightning # Install icevision # !pip install icevision[all] icedata # Import everything from icevision from icevision.all import * import icedata # Load the data and create the parser data_dir = icedata . pennfudan . load_data () class_map = icedata . pennfudan . class_map () parser = icedata . pennfudan . parser ( data_dir ) train_records , valid_records = parser . parse () # Define the transforms and create the Datasets presize = 512 size = 384 shift_scale_rotate = tfms . A . ShiftScaleRotate ( rotate_limit = 10 ) crop_fn = partial ( tfms . A . RandomSizedCrop , min_max_height = ( size // 2 , size ), p = 0.5 ) train_tfms = tfms . A . Adapter ( [ * tfms . A . aug_tfms ( size = size , presize = presize , shift_scale_rotate = shift_scale_rotate , crop_fn = crop_fn , ), tfms . A . Normalize (), ] ) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( size = size ), tfms . A . Normalize ()]) train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # Shows how the transforms affects a single sample samples = [ train_ds [ 0 ] for _ in range ( 6 )] show_samples ( samples , denormalize_fn = denormalize_imagenet , ncols = 3 , label = False , show = True ) # Create DataLoaders train_dl = mask_rcnn . train_dl ( train_ds , batch_size = 16 , shuffle = True , num_workers = 4 ) valid_dl = mask_rcnn . valid_dl ( valid_ds , batch_size = 16 , shuffle = False , num_workers = 4 ) # Define metrics for the model # TODO: Currently broken for Mask RCNN # metrics = [COCOMetric(COCOMetricType.mask)] # Create model model = mask_rcnn . model ( num_classes = len ( class_map )) # Train using pytorch-lightning class LightModel ( faster_rcnn . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-4 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 10 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # BONUS: Use model for inference. In this case, let's take some images from valid_ds # Take a look at `Dataset.from_images` if you want to predict from images in memory samples = [ valid_ds [ i ] for i in range ( 6 )] batch , samples = mask_rcnn . build_infer_batch ( samples ) preds = mask_rcnn . predict ( model = model , batch = batch ) imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs , preds = preds , denormalize_fn = denormalize_imagenet , ncols = 3 )","title":"Using Mask RCNN"},{"location":"examples/quickstart_exp/","text":"Training and End-to-End dataset (Fridge Objects) In this example, we are training the Fridge Objects dataset using either Fastai or Pytorch-Lightning training loop Fastai # pip install icevision[all] icedata from icevision.all import * url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # pip install icevision[all] icedata from icevision.all import * import icedata url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Pytorch Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 70 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Quickstart"},{"location":"examples/quickstart_exp/#training-and-end-to-end-dataset-fridge-objects","text":"In this example, we are training the Fridge Objects dataset using either Fastai or Pytorch-Lightning training loop Fastai # pip install icevision[all] icedata from icevision.all import * url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () # Transforms train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Fastai learn = efficientdet . fastai . learner ( dls = [ train_dl , valid_dl ], model = model , metrics = metrics ) learn . fine_tune ( 50 , 1e-2 , freeze_epochs = 20 ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , ) Pytorch Lightning # pip install icevision[all] icedata from icevision.all import * import icedata url = \"https://cvbp.blob.core.windows.net/public/datasets/object_detection/odFridgeObjects.zip\" dest_dir = \"fridge\" # Loading Data data_dir = icedata . load_data ( url , dest_dir ) # Parser class_map = ClassMap ([ \"milk_bottle\" , \"carton\" , \"can\" , \"water_bottle\" ]) parser = parsers . voc ( annotations_dir = data_dir / \"odFridgeObjects/images/\" , images_dir = data_dir / \"odFridgeObjects/annotations\" , class_map = class_map ) # Records train_records , valid_records = parser . parse () train_tfms = tfms . A . Adapter ([ * tfms . A . aug_tfms ( size = 384 , presize = 512 ), tfms . A . Normalize ()]) valid_tfms = tfms . A . Adapter ([ * tfms . A . resize_and_pad ( 384 ), tfms . A . Normalize ()]) # Datasets train_ds = Dataset ( train_records , train_tfms ) valid_ds = Dataset ( valid_records , valid_tfms ) # DataLoaders train_dl = efficientdet . train_dl ( train_ds , batch_size = 16 , num_workers = 4 , shuffle = True ) valid_dl = efficientdet . valid_dl ( valid_ds , batch_size = 16 , num_workers = 4 , shuffle = False ) # Model and Metrics model = efficientdet . model ( model_name = \"tf_efficientdet_lite0\" , num_classes = len ( class_map ), img_size = size ) metrics = [ COCOMetric ( metric_type = COCOMetricType . bbox )] # Training using Pytorch Lightning class LightModel ( efficientdet . lightning . ModelAdapter ): def configure_optimizers ( self ): return SGD ( self . parameters (), lr = 1e-2 ) light_model = LightModel ( model , metrics = metrics ) trainer = pl . Trainer ( max_epochs = 70 , gpus = 1 ) trainer . fit ( light_model , train_dl , valid_dl ) # Inference # DataLoader infer_dl = efficientdet . infer_dl ( valid_ds , batch_size = 8 ) # Predict samples , preds = efficientdet . predict_dl ( model , infer_dl ) # Show samples imgs = [ sample [ \"img\" ] for sample in samples ] show_preds ( imgs = imgs [: 6 ], preds = preds [: 6 ], class_map = class_map , denormalize_fn = denormalize_imagenet , ncols = 3 , )","title":"Training and End-to-End dataset (Fridge Objects)"}]}